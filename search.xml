<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[(二)End-to-End Recovery of Human Shape and Pose]]></title>
    <url>%2F2019%2F03%2F09%2Fe6413cd%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;End-to-end Recovery of Human Shape and Pose论文链接 Github链接 Contribution 单图重建 利用SMPL进行重建，并且达到real-time效果 不要求每一个训练的image都要有其对应的3D ground truth Method 作者提出了一个end-to-end的网络来从单张RGB人像恢复其3D形状。作者利用已有的SMPL模型，SMPL模型可由3D relative joint rotation和Shape 来刻画一个人的3D shape。作者认为在以往的人体建模工作当中，有3D joint location来估计一个完整的3D shape 是不鲁棒的。原因有二：1、3D joint location alone do not constrain the full DoF at each joint. DoF意思就是景深，这个跟相机的参数有关。对这个参数的估计错误，可能会导致所估计的3D shape在图像中的显示位置有区别（我自己想的，不一定正确）2、Joints are sparse, whereas the human body is defined by a surface in 3D space. 这个就是Joints的点数过少，不足以约束一个完整的3D human shape。 作者还解决了两个在重建工作的问题。其一就是缺少3D ground-truth数据，同时，已有的3D ground-truth绝大部分是在实验室环境下采集的，其对in the wild的2D图像泛化性很差。其二就是单视角情况下2D-3D mapping的问题，不同的3D shape存在对应一张相同的2D图片，在这些3D shape中，存在一些不正常的shape(may not be anthropometrically reasonable) 网络训练 网络结构 Encoder和Regression 网络首先输入一张图片经过Encoder(Resnet-50)后，输出一个feature$\in \mathbb{R}^{2048}$。之后输入到一个Regressor（3层FC,2048D-&gt;1024D-&gt;1024D-&gt;85D）进行迭代，总共会迭代3次，每次迭代过程中会输入一个cam_para,shape $\beta$,pose $\theta$。将pose和shape输入smpl可以得到本次迭代后预测的模型（包括3D joint location）。由cam_para和3D joint location可以得到2D joint location从而可以计算$L_{reproj}$。由pose(也就是joint rotation)和shape可以计算出$L_{adv}$。若输入的2D image有对应的3D数据，则还可以计算$L_{3D}$。 但是在最后计算loss的时候，只会使用每次迭代时产生的$L_{adv}$，而$L_{3D}$和$L_{reproj}$只会利用最后一次迭代产生的loss。这是由于作者认为若每次迭代产生的loss全部都利用,这容易导致regressor限于局部最优化。 Adversarial Prior Adversarial Prior是用来解决上面提到的生成3D数据不真实情况的一个判别器。由于该判别器的任务是判断smpl参数是否是一个正常的人体，那么自然就不需要与输入2D image对应的3D ground truth数据来训练这个判别器判断参数的正确性。由于事先知道我们预测的latent space的意义，所以作者将整个discriminator分解为pose discriminator和shape discriminate。作者又进一步将pose discriminator分解为对每个关节点的判别器（23个，这些判别器的作用在于限制每个关节点的旋转角度）和一个对所有关节点整体做判断的判别器（这个判别器作用在于判断所有关节点的分布关系)。 作者认为因为这个网络结构不存在刻意去欺骗Discriminator的行为，所以不会产生一般GANs网络会产生的mode collapse现象。 loss函数 $L = \lambda(L_{reproj} + \mathbb{1}L_{3D}) + L_{adv} $，$\lambda$是用来控制两个目标函数的相对权重的。若输入的2D图像有对应的ground truth 3D数据时，$\mathbb{1}$的值就为1,否则就是0。 $L_{reproj} = \sum_{i}||v_{i}(x_{i}-\hat{x_{i}})||_{1}$,其中$\hat{x_{i}} = s\Pi(RX(\theta,\beta)) + t$。 $R$表示旋转操作，$\Pi$表示正交投影，$s，t$分别表示Scale和translation。 $L_{3D} = L_{3D joints} + L_{3D smpl}$ $L_{joints} = ||(X_i-\hat{X_i})||_2^2$ $L_{smpl} = ||[\beta_i,\theta_i]-[\hat{\beta_i},\hat{\theta_i}]||_2^2$ $minL_{abv}(E) = \sum_i\mathbb{E}_{\Theta ~ pE}[(D_i(E(I)-1)^2]$ $minL(D_i) = \mathbb{E}_{\Theta~pdata}[(D_i(\Theta)-1)^2]+\mathbb{E}_{\Theta ~ pE}[D_i(E(I)^2]$ 很典型的GANs网络结构的Loss函数，其中$\Theta$表示真实的smpl参数。（这个loss函数真的能够限制每个关节的旋转角度和整体关节的分布情况么？？表示很难理解,欢迎提出新的见解~) Experiments对于2D image dataset,作者使用了LSP,LSP-extended,MPII和MS COCO，训练数据量分别是1k,10k,20k和80k。2D image和3D dataset作者使用了Human 3.6M和MPI-INF-3DHP。而用作训练discriminator的SMPL真实数据是: CMU, Human3.6M training set 和the PosePrior dataset,训练数据量分别是390k,150k,180k。 作者在Human 3.6M上采用了两种评价标准： mean per joint position error (MPJPE) Reconstruction error, which is MPJPE after rigid alignment of the prediction with ground truth via Procrustes Analysis. 相较于第一种，Reconstruction error能排除全局误差的影响，更好的比较3D skeleton. 作者发现即使拥有较高的MPJPE，文中的方法生成的3D模型从视觉效果上看，也不错。 作者在MPI-INF-3DHP上采用了两种评价标准： Percentage of Correct Keypoints(PCK) thresholded at 150mm Area Under the Curve(AUC) over a range of PCK thresholds]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维人体重建</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>深度学习</tag>
        <tag>SMPL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(一)Learning Category-Specific Mesh Reconstruction From Image Collections]]></title>
    <url>%2F2019%2F03%2F08%2Fed6ad9d5%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Learning Category-Specific Mesh Reconstruction from Image Collections论文链接 Github链接 Contribution 单图重建 纹理渲染 训练过程无需3d ground truth数据 Method训练数据预处理本次实验用到的数据集是CUB-200-2011,提供了200种鸟类，共计11788张图片。每张图片附含了15个关键点的标记（头部，背部，脚等）、一张Mask和Bounding box。作者首先将这些数据导入Matlab当中，首先对每组关键点进行零均值处理，接着计算平均模型$\hat{S}$。 接着利用所有关键点进行structure from motion变换，得到每张图片相对于平均模型$\hat{S}$的rot,Translation和Scale三个参数。 网络训练作者用到了三个网络来达到重建效果，分别是文中所用主体网络,Neural Mesh Render和Perceptual loss。 论文中所用网络简介 首先网络接受一张图片，经过encoder之后变为了一个200d的shape feature f（注：encoder是一个经过imagenet预训练的resnet-18接上两个全连接层）。之后将得到的f送入三个网络,分别是ShapePredictor,CameraPredictor和UVMapPredictor. 其中ShapePredictor是预测形状变化$\Delta{V}$,最后的形状预测结果就是$V = \hat{V} + \Delta{V}$。CameraPredictor是预测rot,Translation和Scale三个参数的。值得一提的是rot参数,它与以前见到的旋转矩阵不同，rot$\in \mathbb{R}^4$,是一个四元数,可以用来表示三维空间中点的旋转。 TexturePredictor 作者提出的纹理预测思路比较巧妙。首先在数据预处理过程中的$\hat{S}$与网络训练过程的$\hat{V}$并不一致，实际上$\hat{V} =\mathcal{P}(\hat{S})$，$\mathcal{P}$表示将一个点数为642，面片数为1280的二十面体投影至$\hat{S}$。此前为了利用动物左右身体对称的特性，将上面提到的二十面体做了一个对称处理。使得二十面体的面片种类可以分为两种：1、左右对称 2、中间独立（保持拓扑结构）。而投影操作$\mathcal{P}$作者在代码注释中写了这么一句话： 1234def triangle_direction_intersection(tri, trg): Finds where an origin-centered ray going in direction trg intersects a triangle. Args: tri: 3 X 3 vertex locations. tri[0, :] is 0th vertex. 有了上面的基础，作者就认为由于所有的预测3D模型都是从同一个平均模型在保持拓扑变化的基础上变化得到的原因，所以每个所预测3D模型的点语义都是一致，即编号为1的点若代表嘴巴，则所有预测模型中编号为1的点就是所预测的嘴巴那个点。那么对于所预测的纹理图片来说，只要知道了最原始的那个二十面体的UV图，便可以对预测3D模型进行渲染(rendering)。 Loss函数介绍 注1：$\mathcal{R}$（）和$G$（）分别表示渲染(rendering)和双线性取样(bilinear sampling),其中$\mathcal{R}$（*）引用自Hiroharu Kato, etc. Neural 3D Mesh Renderer 注2：$L_{texture}$函数表示的percetual loss,较传统的pixel loss相比更能从人的感知角度来评价两幅图像的相似度 引用自Zhang,R etc. The unreasonable effectiveness of deep networks as a perceptual metric. In CVPR 2018 $L_1 = L_{reproj} + L_{mask} + L_{cam} + L_{smooth} + L_{def} + L_{vert2kp}$ $L_{reproj} = \sum_{i}^{}||x_{i}-\tilde{\pi}_{i}(AV_{i})||_{2}$，其中 $\tilde{\pi}（*）$表示投影操作，其值是从structure-from-motion中获得参数 $L_{mask} = \sum_i||S_{i}-\mathcal{R}(V_{i},F,\tilde{\pi}_{i})||_{2}$，$S_{i}$表示ground-truth的Mask。 $L_{cam} = \sum_{i}||\tilde{\pi}_{i}-{\pi}_{i}||_{2}$，$\pi_{i}$表示估计的相机旋转参数，由于旋转参数是由四元数表示，那么就是利用hamilton_product来求解估计与实际的误差 $L_{smooth} = ||LV||_{2}$，$L$表示Laplacian光滑，其目的是为了最小化平均曲率 $L_{def}=||\Delta{V}||_{2}$，这个是一个正则项 $L_{vert2kp} = \frac{1}{|K|}\sum_{k}\sum_{v}-A_{k,v}logA_{k,v}$，这个是一个k*V的矩阵，每一行表示一个特征点，而每一列表示这个特征点在各个点的概率分布。初始化是，每个特征点在各个点上的概率分布是一致的，进过迭代之后，作者期望形成一个类似与one-hot的矩阵。 $L_2 = L_{texture} + L_{dt}$ $L_{texture} = \sum_{i}dist(\mathcal{S_{i}}\bigodot\mathcal{I_{i}},\mathcal{S_{i}}\bigodot\mathcal{R}(V_{i},F,\tilde{\pi_{i}},I^{uv}))$ $L_{dt} = \sum_{i}\sum_{u,v}G(\mathcal{D_{S_{i}};F_{i}}(u,v)$， $\mathcal{D_{S_{i}}}$表示对于每一个Mask的distance transform。 TexturePredictor会输出一个texture flow，也就是$\mathcal{F} \in \mathbb{R}^{H_{uv} * W_{uv} * 2}$,其中$H_{uv}*W_{uv}$分别表示UVmap的长和宽，而$\mathcal{F}(x,y)$对应的就是input image(x,y)。最后的UVmap就是$I^{uv} = G(I;\mathcal{F})$。而这个texture flow不是神经网络的直接输出产物，而神经网络直接输出的是一个表示体现了UV图和Img的对应位置的关系，而其还是会与另一个UVsample进行取样。而UVsample表示的是UV图与$\hat{V}$上各个点的一个对应关系。这两个东西结合在一起后就可以体现UVmap、$\hat{V}$、img三者之间的关系。 Experiments]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维动物重建</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>纹理渲染</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F07%2F4a17b156%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>

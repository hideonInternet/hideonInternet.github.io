<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[C++中浅拷贝和深拷贝]]></title>
    <url>%2F2019%2F12%2F24%2Fda5b8819%2F</url>
    <content type="text"><![CDATA[C++中类的拷贝有两种：深拷贝，浅拷贝：当出现类的等号赋值(即：a=b)时，即会调用拷贝函数 深拷贝和浅拷贝的区别 在未显示定义拷贝构造函数–（默认为浅拷贝），它能够完成成员的一一复制。当数据成员中没有指针时，浅拷贝是可行的。但当数据成员中有指针时，如果采用简单的浅拷贝，则两个对象的数据成员中的指针变量将指向同一个地址。当其中一个对象结束调用时，调用析构函数，指针所指的内存被释放，但是此时另要给对象中的指针还指向那被释放的内存块，这就导致指针悬挂现象。 拷贝构造函数是一种特殊的构造函数，它在创建对象时，是使用同一类中之前创建的对象来初始化新创建的对象。拷贝构造函数通常用于： - 通过使用另一个同类型的对象来初始化新创建的对象。 - 复制对象把它作为参数传递给函数。 - 复制对象，并从函数返回这个对象。 构造拷贝函数最常见的形式：1234// https://www.runoob.com/cplusplus/cpp-copy-constructor.htmlclassname (const classname &amp;obj) &#123; // 构造拷贝函数的主体&#125; 深拷贝与浅拷贝的区别就在于深拷贝会在堆内存中另外申请空间来储存数据，从而也就解决了指针悬挂的问题。简而言之，当数据成员中有指针时，必须要用深拷贝。 应用场景123456789101112// 浅拷贝， 成员变量中没有动态分配的资源class A &#123; public: A(int _data) : data(_data)&#123;&#125; A()&#123;&#125;private: int data; &#125;;int main() &#123; A a(5); b = a; // 仅仅是数据成员之间的赋值 &#125; 假设我们成员变量中有指针指向堆(heap)的时候，因为堆的空间属于动态分配。若使用浅拷贝，则两个指针变量是指向同一块堆内存区域。 123456789101112131415161718192021222324class A &#123; public: A(int _size) : size(_size) &#123; data = new int[size]; &#125; // 假如其中有一段动态分配的内存 A()&#123;&#125;; ~A() &#123; delete [] data; &#125; // 析构时释放资源private: int* data; int size; &#125;int main() &#123; A a(5), b = a; // 注意这一句 /* b.size = a.size; b.data = a.data; // 注意这里，就会出现上面这种情况。 */&#125; 这时候就需要深拷贝123A(const A&amp; _A) : size(_A.size)&#123; data = new int[size];&#125; // 深拷贝 new和不用new创建类对象区别 new创建的类对象需要指针接受，一处初始化，多处使用。对象使用完需delete销毁。 new创建类对象直接使用堆空间，而局部不用new定义类对象则使用栈空间 1234ClassName* a = new ClassNet();delete a // 需要显式调用delete删除a所指内存ClassName a; //不用new， 直接使用类定义声明，由于是存储在栈中，使用完后不需要手动释放。 new对象指针可以作为函数返回值，函数形参。 实例关于深拷贝的使用，可以参考leetcode 138题]]></content>
      <categories>
        <category>c++编程基础</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>浅拷贝，深拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中栈和堆的区别]]></title>
    <url>%2F2019%2F12%2F24%2F93b6f39e%2F</url>
    <content type="text"><![CDATA[预备知识一个由C/C++编译的程序占用的内存分为以下几个部分 栈（stack) 由编译器自动分配释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈 堆（heap) 由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收。其与数据结构中的堆不相同，分配的方式类似于链表。 全局区（静态区，static）全局变量和静态变量的存储是放在一块的，初始化的全局变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 文字常量区 常量字符串的存储位置。程序结束后由系统释放 程序代码区 存放函数体的二进制代码 123456789101112131415161718192021//main.cppint a = 0; 全局初始化区char *p1; 全局未初始化区main()&#123;int b; 栈char s[] = &quot;abc&quot;; // 注意：abc 分配在静态存储区，不是栈上char *p1; //栈char *p2; //栈char *p3 = &quot;123456&quot;; //123456\0在常量区，p3在栈上。static int c =0； 全局（静态）初始化区// 堆区。p1 = (char *)malloc(10); // c// 两种方法的区别// https://stackoverflow.com/questions/3902011/whats-the-difference-between-new-char10-and-new-char10p2 = new char[10]; // c++p2 = new char(10); // c++strcpy(p1, &quot;123456&quot;); // 123456\0放在常量区，编译器可能会将它与p3所指向的&quot;123456&quot;优化成一个地方。&#125; 注意在上面的示例中，指针p1,p2本身是存储在栈中的，但是其所指的地址是指向堆的。这一点要分清楚。 申请后系统的响应 栈 (stack)： 只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。 堆 （heap): 首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序，另外，对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中 申请大小的限制 栈 (stack)： 在Windows下,栈是向低地址扩展的数据结构，是一块连续的内存的区域。这句话的意思是栈顶的地址和栈的最大容量是系统预先规定好的，在WINDOWS下，栈的大小是2M（也有的说是1M，总之是一个编译时就确定的常数），如果申请的空间超过栈的剩余空间时，将提示overflow。因此能从栈获得的空间较小。 堆 （heap): 堆是向高地址扩展的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。 申请效率的比较 栈 (stack)： 栈由系统自动分配，速度较快。但程序员是无法控制的。 堆 （heap): 堆是由new分配的内存，一般速度比较慢，而且容易产生内存碎片,不过用起来最方便。 堆和栈中的存储内容 栈 (stack)： 在函数调用时，第一个进栈的是主函数的下一条指令（函数调用语句的下一条可执行语句）的地址，然后是函数的各个参数，在大多数的C编译器中，参数是由右往左入栈的，然后是函数中的局部变量。注意静态变量是不入栈的。当本次函数调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向最开始存的地址，也就是主函数中的下一条指令，程序由该点继续运行。 堆 （heap): 一般是在堆的头部用一个字节存放堆的大小。堆中的具体内容有程序员安排。 总结堆和栈的区别可以总结为一下几点： 管理方式不同: 对于栈来讲，是由编译器自动管理，无需我们手工控制；对于堆来说，释放工作由程序员控制，容易产生memory leak。 空间大小不同: 一般来讲在32位系统下，堆内存可以达到4G的空间，从这个角度来看堆内存几乎是没有什么限制的。但是对于栈来讲，一般都是有一定的空间大小的，例如，在VC6下面，默认的栈空间大小是1M（好像是，记不清楚了）, 可以修改。 能否产生碎片不同： 对于堆来讲，频繁的new/delete势必会造成内存空间的不连续，从而造成大量的碎片，使程序效率降低。对于栈来讲，则不会存在这个问题， 因为栈是先进后出的队列，他们是如此的一一对应，以至于永远都不可能有一个内存块从栈中间弹出，在他弹出之前，在他上面的后进的栈内容已经被弹出。 生长方向不同：对于堆来讲，生长方向是向上的，也就是向着内存地址增加的方向；对于栈来讲，它的生长方向是向下的，是向着内存地址减小的方向增长。 分配方式不同：堆都是动态分配的，没有静态分配的堆。栈有2种分配方式：静态分配和动态分配。静态分配是编译器完成的，比如局部变量的分配。动态分配由alloca函数进行分配，但是栈的动态分配和堆是不同的，他的动态分配是由编译器进行释放，无需我们手工实现。 分配效率不同：栈是机器系统提供的数据结构，计算机会在底层对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。堆则是C/C++函数库提供的，它的机制是很复杂的，例如为了分配一块内存，库函数会按照一定的算法（具体的算法可以参考数据结构/操作系统）在堆 内存中搜索可用的足够大小的空间，如果没有足够大小的空间（可能是由于内存碎片太多），就有可能调用系统功能去增加程序数据段的内存空间，这样就有机会分 到足够大小的内存，然后进行返回。显然，堆的效率比栈要低得多。 Reference:https://blog.csdn.net/wo17fang/article/details/52244238]]></content>
      <categories>
        <category>c++编程基础</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>栈</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(八)Occupancy Networks Learning 3D Reconstruction in Function Sapce]]></title>
    <url>%2F2019%2F12%2F21%2Fb040bbb5%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Occupancy Networks: Learning 3D Reconstruction in Function Sapce论文链接 github Contribution 作者在文中提出了一种新的3D表征方法，并证明了该表征方法可以用于高效的三维重建工作。 作者提出了Multiresolution IsoSurface Extraction (MISE)用以从函数空间中恢复高精度的三维模型以较低的计算代价。 Backgroud作者认为主流的3D表征方法有着一下的不足： VoxelVoxel是用规则的立方体来表示三维形状的。若想要提高Voxel的精度，所耗费的计算量是以三次方速度增长的，尽管有像Octree这种方法来减少计算量，但精度还是限制了$256^3$上边。 Point Cloud利用点来表示三维形状，与Voxel相比可以大幅度提高精度。但是由于缺少点与点之间的连接关系，就看上去就没有那么的直观。同时缩放性也较差。 Mesh通过增加点与点之间的连接关系，弥补了点云的不足。但是其拓扑结构不能改变，而如今基于Mesh表示三维重建工作，都是通过预测一个初始形状的deformation来进行三维重建，这就使得所预测的三维形状必须具有相同的拓扑结构（比方说一个空心圆圈和一个实心圆饼的拓扑结构就不一致） Method网络通过输入一个$p(x,y,z) \in \mathbb R^3$和一张图片或者点云或者体素$x \in \cal X$得到$p$点是否在物体内部的一个概率值probability of occupancy。 对于不同的类型的输入，作者使用不同的编码器来编码特征。 图片 -&gt; Resnet 18 点云 -&gt; PointNet encoder 体素 -&gt; 3D convolutional neural network $f_{\theta}: \mathbb R^3 \times \cal X \rightarrow [0,1] \tag{1}$ 从作者开源的代码来看，网络的decoder是一个输出维度为1的全连接层，我们将这个输出记为logits。在计算loss的时候，这个logits就与真实的occupancy value进行比较。 但是在评测IoU或者进行可视化的时候，就要用sigmoid(logits)来求得一个介于[0,1]之间的概率值，最后利用marching cubes进行可视化。 损失函数的定义也十分简单，就是一个交叉熵函数: $\cal L_{\cal B}(\theta) = \frac{1}{|\cal B|}\sum^{|\cal B|}{i=1}\sum^{K}{j=1}\cal L(f_{\theta}(p_{ij},x_{i}),o_{ij}) \tag{2}$ 其中$o_{ij}$表示真实的occupancy value Occupancy Network Architecture 作者没有直接将encoder出来的特征向量c与输入的三维(x,y,z)坐标进行concat然后直接送入MLP进行映射，而是通过c来计算出batchnorm是denormalization所需要的scale和bias，以这种方式来影响最后pred-occupancy value的预测。作者在试验中也给出是否使用Conditional batch normalization的对比结果。 KL散度作者在文中提到: Our 3D representation can also be used for learning probabilistic latent variable models 作者首先利用两个全连接来回归mean和log(std)（回归$\log(std)$是为了防止std为非正数）。1234567891011121314mean, logstd = encoder_latent(p, occ, c) # q_z 是一个正态分布 q_z = dist.Normal(mean, torch.exp(logstd)) # 随机采样一个z z = q_z.rsample() # 计算kl散度，使得编码mean, logstd的编码器尽量靠近正态分布，以保证张成空间的 # 多样性。假设不加这个散度，为了让decoder每次生成的形状越来越好，那么每次送入 # 的值都最好一致，那么logstd会逐渐变得很小，使得分布的标准差很小接近0。这样每 # 次采样的结果z都近乎一致， 使得latent space并不能真的代表潜在空间。 kl = dist.kl_divergence(q_z, p0_z).sum(dim=-1) # logits即为网络最终预测值，要与真实occupancy value进行比较。 # 评估重建效果时，将sigmoid(logits)送入marching cubes进行重建 logits = decode(p, z, c) 由以上代码可知，最终重建的结果收到两个输入的影响,z 和 c。其中z是从表示c的分布中采样得到的，可以控制一定的形变。 可以看到，通过改变采样的z可以得到不同的形状。 Multiresolution IsoSurface Extraction (MISE)假如我们已经学习到了一个函数空间$f_{\theta}$, 最直接的方式的采样的到具有$256^3$个点的grid, 然后得到每个点的pre-occupancy value，然后送入marching cube进行重建，但是这样大大限制了重建的精度。感觉上作者是借鉴的Octree的思想，逐步求解，最后达到预定分辨率。 ExperimentsUpper bound作者首先评估了在函数空间中表示一个三维物体的可能性和其准确值，该准确值可以作为一个upper-bound来比较接下来的实验结果。由于是找到一个upper-bound，所以这个实验的训练个测试都是在训练集上进行的。实验步骤如下： 一： 将训练集中的所有图片（4746张）映射为一个长度为512维的特征向量，表征为latent space中的一个点，在训练过程中，图片-&gt;特征向量的映射关系保持不变。二： 用一个decoder将512维的特征向量转换为一个1维的pred-occupancy value, 然后进行重建。 可以看到，本文所提出的表征方法，其重建精读（IoU）并不会随着分辨率的提高有太大的波动，始终保持在一个很高的水平(0.89)。同时，其网络参数也不会发生变化。反观基于Voxel的方法，虽然重建精度随着分辨率的提高有较大提升，但是其网络参数的增加量却是cubic。 Completion and Super-Resolution正如前面提到的，网络Encoder的输入并不一定要是图片，还可以体素(Voxel), 点云(point cloud)。除了对图片所进行的单图重建任务（有3D监督）外，还对点云的补全和体素的超分都做了相应的实验。 Ablation Study 这里我们更关心的是第三行No CBN的对比结果。如果不使用Conditional Batch Normalization可以看到结果下降了比较大的幅度。具体到结构上的改变是，对于BN, 作者先是用一个全连接将输入的特征向量c映射为一个定好的长度(设为F1), 接着作者将待预测的点p(x,y,z)映射为相同的长度(F1) 然后两者相加后，在送入一维卷积+BN， 最后得到pred-occupancy value。 疑问Q:真实的occupancy value是如何取得的，在源码1和源码2可以看到评估和计算loss用到的网络输入确实是差一个sigmoid函数。但是真实的occ是如果求解的？ A: 在补充材料中，作者表示一个点真实的occupancy value的获取与其穿越的面片数有关。具体来讲，做一条与z轴平行且以p点为起点的直线，记为A, 若A穿过的面片数为偶数，则其在物体外部。反之，若A穿过面片数位奇数，则其在物体内部。 Q 论文中提到点的法向量可以通过求导得到，如何操作？ In addition, our approach also allows to efficiently extract normals for all vertices of our output mesh by simply backpropagating through the occupancy network. Q 其实对于特征融合，像作者在Ablation Study这样，将两个没有很强关联的特征直接进行相加，然后卷积得到结果的情况是比较少见的。更多的是特征之间的Concat, 或者像GEOMetrics这篇文章一样，保留一部分。比较失望作者没有考虑到这个很明显的问题。]]></content>
      <categories>
        <category>论文阅读</category>
        <category>刚性物体重建</category>
      </categories>
      <tags>
        <tag>函数空间</tag>
        <tag>Marching Cubes</tag>
        <tag>KL散度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见排序算法]]></title>
    <url>%2F2019%2F12%2F05%2Fec44b182%2F</url>
    <content type="text"><![CDATA[2019年12月18日更新：加入归并排序的两种链表版本：top-down, bottom-up。 冒泡排序123456789101112131415//冒泡排序: 每次迭代最大的在最上方 int* sort(int* nums, int numsSize)&#123; int tmp = 0; // 注意这里的起始值 for(int i =numsSize-1;i&gt;0;i--)&#123; //最大的值所放置的位置 for(int j = 0;j&lt;i;j++)&#123; // 比较的位置 if(nums[j] &gt; nums[j+1])&#123; tmp = nums[j]; nums[j] = nums[j+1]; nums[j+1] = tmp; &#125; &#125; &#125; return nums;&#125; 时间复杂度：$O(n^2)$ 空间复杂度：$O(1)$ 稳定 选择排序123456789101112131415161718// 选择排序 找到最大或最小的值，放在最边的位置 int* sort(int* nums, int numsSize)&#123; int minInd = 0; int tmp = 0; for(int i=0;i&lt;numsSize-1;i++)&#123; //最小值的位置 和 比较的位置 minInd = i; for(int j =i+1;j&lt;numsSize;j++)&#123; if(nums[minInd] &gt; nums[j])&#123; minInd = j; &#125; &#125; // 交换 tmp = nums[i]; nums[i] = nums[minInd]; nums[minInd] = tmp; &#125; return nums; &#125; 时间复杂度：$O(n^2)$ 空间复杂度：$O(1)$ 不稳定 插入排序123456789101112131415// 插入排序 每次迭代，都与以排序好的数组进行比较，在排序好的数组中找到合适的位置。 int* sort(int* nums, int numsSize)&#123; for(int i=0;i&lt;numsSize;i++)&#123; int tmp = nums[i]; int j =i; while(j&gt;0 &amp;&amp; tmp &lt; nums[j-1])&#123; nums[j] = nums[j-1]; j=j-1; &#125; nums[j] = tmp; &#125; return nums; &#125; 时间复杂度：$O(n^2)$ 空间复杂度：$O(1)$ 稳定 归并排序 数组Top-Down版本 1234567891011121314151617181920212223242526272829303132333435363738394041// 归并排序 分治算法void merge(int* nums, int start, int mid, int end)&#123; // for(int i=start;i&lt;=end;i++)&#123; // printf(&quot;%d &quot;,nums[i]); // &#125; // printf(&quot;\n&quot;); int p =start, q =mid+1; int* newNums = (int*)malloc(sizeof(int) * (end-start+1)); int k =0; for(int i = start;i&lt;=end;i++)&#123; // 检查第一部分是否完毕 if(p &gt; mid) newNums[k++] = nums[q++]; else if(q &gt; end) newNums[k++] = nums[p++]; else if(nums[p] &lt; nums[q]) newNums[k++] = nums[p++]; else newNums[k++] = nums[q++]; &#125; for(int p=0;p&lt;k;p++)&#123; nums[start++] = newNums[p]; &#125; &#125;void divide(int* nums, int start, int end)&#123; if(start &lt; end)&#123; int mid = (start + end) / 2; divide(nums,start,mid); divide(nums, mid+1, end); merge(nums, start , mid ,end); &#125;&#125;int* sort(int* nums, int numsSize)&#123; divide(nums, 0, numsSize-1); return nums;&#125; 时间复杂度：$O(n\log_2{n})$ 空间复杂度：$O(n+log_2{n})$ 稳定 链表Top-Down版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123;public: ListNode* sortList(ListNode* head) &#123; if(!head || !head-&gt;next) return head; /* 这里head-&gt;next是要给trick, 因为如果不加next的话,若子链长度为2,在 一次walk之后，fast指向nullptr,slow就指向最后一个元素，而下边有mid =slow-&gt;next = nullptr 所以就直接返回了，等于只有左半边链。而这时做半边脸仍然有两个元素，就会陷入循环，导致无法正常结束。 换句话说，如果不加next, 当子链长度为2时，就无法继续进行分治（分为左右两个长度为1的子链）,使得程序无法结束。 而又由于我们使用递归，使得栈不断有新数据写入，最后就栈满溢出。 */ ListNode* slow = head; ListNode* fast = head-&gt;next; // 当fast走到低时，slow正好在中间 while(fast &amp;&amp; fast-&gt;next)&#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; &#125; ListNode* mid = slow-&gt;next; // 将左，右两个子链断开 slow-&gt;next = nullptr; return merge(sortList(head), sortList(mid)); &#125;private: // 两个子链排序，空间复杂度为常数 ListNode* merge(ListNode* l1, ListNode* l2)&#123; ListNode dummy(0); ListNode* tail = &amp;dummy; while(l1 &amp;&amp; l2)&#123; if(l1-&gt;val &gt; l2-&gt;val) swap(l1,l2); tail-&gt;next = l1; l1 = l1-&gt;next; tail = tail-&gt;next; &#125; if(l1) tail-&gt;next = l1; if(l2) tail-&gt;next = l2; return dummy.next; &#125;&#125;; 时间复杂度：$O(log_2{n})$ 空间复杂度$(log_2{n})$ 稳定 链表Bottom-up版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class Solution &#123;public: ListNode* sortList(ListNode* head) &#123; if(!head || !head-&gt;next) return head; //由于我们采用bottom-up,所以需要知道整个链表的长度，作为大致范围的限制，但要求不算严格 int len = 0; ListNode* cur = head; // 获取长度 while(cur = cur-&gt;next) ++len; ListNode dummy(0); dummy.next = head; ListNode* l; ListNode* r; ListNode* tail; for(int n =1; n&lt;len; n&lt;&lt;=1)&#123; /* cur不能等于head, 因为head只是一个存储值得指针名，并不是代表 它在每次排序之后，都是头结点，可能在一次排序过后，head指针所 指的那个元素，就被放置在了最尾处，所以不能每次都将起始位置初始化为head. 这也是为什么要维护一个dummy结点，使dummy结点每次都指向真正的头节点。 */ // cur = head; cur = dummy.next; tail = &amp;dummy; while(cur)&#123; l = cur; r = split(l, n); cur = split(r, n); auto merged = merge(l, r); tail-&gt;next = merged.first; tail = merged.second; &#125; &#125; return dummy.next; &#125;private: // 前n个元素一组，剩下的元素一组，返回剩下元素的head ListNode* split(ListNode* head, int n)&#123; // 这里需要注意的是子链长度n是包括了头结点的长度 while(--n &amp;&amp; head) head = head-&gt;next; ListNode* rest = head ? head-&gt;next : nullptr; if(head) head-&gt;next = nullptr; // 上面两行或换成下面易懂 // if(head)&#123; // rest = head-&gt;next; // head-&gt;next = nullptr; // &#125;else&#123; // rest = nullptr; // &#125; return rest; &#125; pair&lt;ListNode*, ListNode*&gt; merge(ListNode* l1, ListNode* l2)&#123; ListNode dummy(0); ListNode* tail = &amp;dummy; while( l1 &amp;&amp; l2)&#123; if(l1-&gt;val &gt; l2-&gt;val) swap(l1,l2); tail-&gt;next = l1; l1 = l1-&gt;next; tail = tail-&gt;next; &#125; tail-&gt;next = l1 ? l1 : l2; while( tail-&gt;next) tail = tail-&gt;next; return &#123;dummy.next, tail&#125;; &#125;&#125;; 时间复杂度：$O(log_2{n})$ 空间复杂度：$O(1)$ 首先对于Top-Down版本来讲，数组版本由于每次需要新建一个数组来对左右两部分子数组进行排序，所以相较于链表版本来说要多$O(n)$的空间复杂度。而对于Top-Down的版本，无论是数组还是链表，都比Bottom-up版本多了递归操作，所以空间复杂度要多$O(log_2{n})$。 快速排序1234567891011121314151617181920212223242526272829303132333435363738394041int partition(int* nums, int start, int end) &#123; int pivot = start; int l = pivot + 1; int r = end; int tmp = 0; // 等号是防止长度为2的子序列 while (l &lt;= r) &#123; while (l &lt;= end &amp;&amp; nums[l] &lt;= nums[pivot]) &#123; ++ l; &#125; while (r &gt;= start &amp;&amp; nums[r] &gt; nums[pivot]) &#123; -- r; &#125; if (l &lt; r) &#123; tmp = nums[l]; nums[l] = nums[r]; nums[r] = tmp; &#125; &#125;; // 这里由于pivot是取到最*左边*一个，要保证交换后，pivot左侧必须全部小于pivot，右侧必须全部大于pivot，就必须 // 与右指针所指元素进行交换（因为这时候右指针所指元素（记为i）小于pivot,交换之后，i还是在pivot*左边*，这就符合条件。 // 假设与左指针进行交换，那么由于做指针所指元素大于pivot，交换之后，在pivot左边就有一个大于pivot的元素，不符合条件。 tmp = nums[pivot]; nums[pivot] = nums[r]; nums[r] = tmp; pivot = r; return pivot;&#125;void quick_sort(int* nums, int start, int end)&#123; if(start &lt; end)&#123; int piv_pos = partition(nums, start, end); quick_sort(nums,start,piv_pos-1); // sorts the left side of pivot quick_sort(nums, piv_pos+1,end); &#125;&#125;int* sort(int* nums, int numsSize)&#123; quick_sort(nums,0,numsSize-1); return nums;&#125; 时间复杂度：$O(n\log_2{n})$ 空间复杂度：$O(1)$ 不稳定]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>冒泡</tag>
        <tag>插入</tag>
        <tag>归并</tag>
        <tag>快速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欧式空间的旋转表示(一)]]></title>
    <url>%2F2019%2F09%2F28%2F4767f891%2F</url>
    <content type="text"><![CDATA[基本概念刚体变换刚体变换也被称为欧氏变换，它在变化当中保持了向量的长度和相互之间的夹角大小，相当于我们把一个刚体原封不同的进行了移动或旋转，不改变它自身的样子。 点与向量的关系点和向量两者并不是等价的关系。向量$\overrightarrow{a}$是一个空间上的抽象概念，只有当我们将其与某个坐标系联系起来的时候，它就具象化为了坐标点。$\begin{align}\overrightarrow{a} =\begin{bmatrix} e_1&amp;e_2&amp;e_3 \end{bmatrix} \begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}= a_1e_1 + a_2e_2 + a_3e_3 \end{align}$ 这里$e_i$为列向量，代表了空间当中坐标系的一组基。 内积和外积 内积 $$\begin{align}a \cdot b = a^Tb = \sum_{i=1}^{3}a_ib_i = |a||b|\cos&lt;a,b&gt; \end{align}$$ 外积 $\begin{align}a \times b = \begin{bmatrix} i&amp;j&amp;k\\a_1&amp;a_2&amp;a_3\\b_1&amp;b_2&amp;b_3 \end{bmatrix} = \begin{bmatrix} a_2b_3 - a_3b_2\\a_3b_1 - a_1b_3\\a_1b_2 - a_2b_1\end{bmatrix} = \begin{bmatrix} 0&amp;-a_3&amp;a_2\\a_3&amp;0&amp;-a_1\\-a_2&amp;-a_1&amp;0 \end{bmatrix}b \triangleq a\^ b \end{align}$ 这里将$\overrightarrow{a}\^$记为将向量$\overrightarrow a$变为反对称。外积的大小为$|a||b|\sin&lt;a,b&gt;$,是两个向量张成的四边形的有向面积。 欧拉角和旋转向量假设我们有一个经过原点的旋转轴$\overrightarrow{u} = (x,y,z)^T$,我们希望将一个向量$\overrightarrow{v}$，以$\overrightarrow u$为旋转轴，旋转$\theta$度之后，变换到$\overrightarrow{v \prime }$ 这时，我们就可以用一个旋转向量$\overrightarrow u$和角度$\theta$来表示一次旋转。由于以这种方式表示旋转需要四个自由度(向量三个，角度一个)，而刻画一次选择只需要三个自由度（x,y,z)这就导致了冗余。所以，我们额外规定向量的模长为一，$|\overrightarrow u| = \sqrt{x^2 + y^2 + z^2} =1$。 欧拉角其实与旋转向量十分类似，但是却更加符合人们思考旋转的方式。欧拉角使用三个分离的转角，把一个旋转分解为三次绕不同轴的旋转。分解方式可以是多种多样的，但通常是分解为X轴，Y轴，Z轴。 绕物体Z轴旋转，得到偏航角yaw 绕旋转之后的Y轴旋转，得到俯仰角pitch 绕旋转之后的X轴旋转，得到滚转角roll 那么一个三维向量[90,-60,80]，就可以定义一次旋转了。但是使用欧拉角会出现万向锁（Gimbal Lock）的错误: 在俯仰角为$±90^°$的时候，第一次旋转与第三次旋转将会使用同一个轴，这使得系统丢失了一个自由度。 旋转矩阵在基本概念中提到，向量在刚体变化中的长度和相对夹角是不变的，利用这种不变性，我们可以得到： $\begin{align}\begin{bmatrix}e_1&amp;e_2&amp;e_3\end{bmatrix}\begin{bmatrix}x_1\\y_1\\z_1\end{bmatrix} &amp; = \begin{bmatrix}e_1^{‘}&amp;e_2^{`}&amp;e_3^{‘}\end{bmatrix}\begin{bmatrix}x_2\\y_2\\z_2\end{bmatrix} \\\begin{bmatrix}x_1\\y_1\\z_1\end{bmatrix}&amp; = \underbrace{\begin{bmatrix}e_1^Te_1^{‘}&amp;e_1^Te_2^{‘}&amp;e_1^Te_3^{‘} \\ e_2^Te_1^{‘}&amp;e_2^Te_2^{‘}&amp;e_2^Te_3^{‘} \\e_3^Te_1^{‘}&amp;e_3^Te_2^{‘}&amp;e_3^Te_3^{‘}\end{bmatrix}}_{R} \begin{bmatrix}x_2\\y_2\\z_2\end{bmatrix}\end{align} $ 我们将中间这个矩阵称为旋转矩阵R,它具有如下性质：$\begin{align} SO(n) = \lbrace R \in \mathbb R^{n \times n} | RR^T = I, det(R) = 1 \rbrace \end{align} $ 不管多少维的旋转矩阵，它都必须是正交矩阵，且满足公式6。同时满足$tr(R) = 2\cos\theta + 1$,$\theta$为旋转的角度。而旋转矩阵的特征值为$\lbrace e^{i\theta},e^{-i\theta},1\rbrace$。 可以根据$R\overrightarrow{n} = R$求解出旋转轴向量。 四元数复数基础如果有两个复数$z_1 = a + bi,z_2 = c +di$,由分配率我们可得 $\begin{align}z_1z_2 &amp;= ac+adi+bci+bdi^2 \\ &amp;= ac-bd+(bc+ad)i \\ &amp;= \begin{bmatrix}a&amp;-b\\b&amp;a \end{bmatrix}\begin{bmatrix}c\\d \end{bmatrix}\end{align}$ 我们可以将$\begin{bmatrix}a&amp;-b\\b&amp;a \end{bmatrix}$ 看成一个二维旋转矩阵，将复平面上的向量$(c,d)$变换到$(ac-bd,bc+ad)$上。具体来说： $\begin{align}\begin{bmatrix}a&amp;-b\\b&amp;a \end{bmatrix} &amp;= \sqrt{a^2 + b^2}\begin{bmatrix}\frac{a}{\sqrt{a^2 + b^2}}&amp;\frac{-b}{\sqrt{a^2 + b^2}}\\\frac{b}{\sqrt{a^2 + b^2}}&amp;\frac{a}{\sqrt{a^2 + b^2}} \end{bmatrix} \\ &amp;=\sqrt{a^2 + b^2} \begin{bmatrix}\cos(\theta)&amp;-\sin(\theta)\\\sin(\theta&amp;\cos(\theta) \end{bmatrix}\end{align}$ 根据欧拉公式，我们可以进一步将二维旋转坐标写成极坐标的形式： $\begin{align}\begin{bmatrix}\cos(\theta)&amp;-\sin(\theta)\\\sin(\theta&amp;\cos(\theta) \end{bmatrix} = \cos(\theta) + i\sin(\theta) = e^{i\theta}\end{align}$ 注意这里的$\theta$是逆时针方向,同时多个二维旋转操作可以叠加，即若干个二维旋转矩阵相乘仍是一个旋转矩阵，而且与施加的次序无关。角度为多个旋转角之和 四元数定义与操作所有四元数$q \in \mathbb H$都可以写成如下形式：$\begin{align} q = a + bi +cj +dk&amp;(a,b,c,d \in \mathbb R)\end{align}$ 其中： $\begin{align}i^2=j^2=k^2=ijk=-1\end{align}$ 通常可以简化表示为有序对形式： $\begin{align}q=[s,\bf{v}]&amp;&amp;(\bf{v}=\begin{bmatrix}x\\y\\z\end{bmatrix},s,x,y,z \in \mathbb R) \end{align}$ 如果一个四元数的$s==0$,即$q_0=[0,{\bf{v}}]$。那么我们就称$q_0$为纯四元数。两个纯四元数相乘:$\begin{align}vu = [\bf -v \cdot u,v \times u]\end{align}$ 四元数的加减法与复数保持一致，乘法可以总结为下表： 与复数一致，四元数的乘法也能写成矩阵乘向量的形式： $\begin{align}q_1q_2 &amp;= \begin{bmatrix}a&amp;-b&amp;-c&amp;-d\\b&amp;a&amp;d&amp;-c\\c&amp;-d&amp;a&amp;b\\d&amp;c&amp;-b&amp;a\end{bmatrix}\begin{bmatrix}e\\f\\g\\h\end{bmatrix}\\&amp;=(ae-(bf+cg+dh))+ \\&amp;(be+af+ch-dg)i+ \\&amp;(ce+ag+df-bh)j+ \\&amp;(de+ah+bg-cf)k \\&amp;=[ae-{\bf{u}} \cdot {\bf{v}}, a {\bf{u}}+e {\bf{v}}+{\bf{v}} \times \bf{u}]\end{align}$ 其中：${\bf{v}} = \begin{bmatrix}b\\c\\d\end{bmatrix},{\bf{u}} = \begin{bmatrix}f\\g\\h\end{bmatrix}$ 注意四元数并不满足交换律，即$q_1q_2 \neq q_2q_1$。这点与复数不一样。 四元数的逆和共轭四元数共轭：$q^* = (s,-{\bf{v}})$ 四元数的逆：$q^{-1} = \frac{q^*}{||q||^2}$ 四元数的逆和共轭都满足交换律 : $q^*q = qq^*=||q||^2\quad;\quad q^{-1} =q^{-1}q=1$ 单位四元数满足 : $q^{-1} = \frac{q^*}{||q||^2} = q^{*}$ 四元数型3D旋转任意向量$\bf v$沿着以单位向量定义的旋转轴$\bf u$旋转$\theta$度之后的$\bf v’$可以使用四元数乘法获得.用纯四元数$v = [0,{\bf v}]$表示三维空间中一个点,$\quad q = [\cos(\frac{1}{2}\theta),\sin(\frac{1}{2}\theta{\bf u})]$表示一次选择，那么： $\begin{align}v’ = qvq^* = qvq^{-1}\end{align}$ 参考资料 视觉SLAM十四讲 euclideanspace quaternion]]></content>
      <categories>
        <category>三维基础</category>
        <category>旋转表征</category>
      </categories>
      <tags>
        <tag>四元数</tag>
        <tag>旋转矩阵</tag>
        <tag>轴角</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(七)Conditional Single-View Shape Generation for Multi-View Stereo Reconstruction]]></title>
    <url>%2F2019%2F09%2F09%2Fdc749c90%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction论文链接 github Contribution 首次提出将单图重建工作中的模糊性进行建模。以单张图片作为输入，网络可以重建出若干符合单视角约束条件的三维模型。而这若干个三维模型就对单视角中的模糊性问题进行了建模，生成了模糊性空间。 Method Overview从上图可以看出，对于单图作为输入的重建模型，现有方法只能重建出一个确定的模型，而由于单图存在重建模糊性问题，肯定会导致该重建结果存在一些问题，特别是对不可见趋于的重建。而作者所提出的方法，可以在符合仅有的单视角约束条件下，重建出若干张图片，这些图片就模拟了不可见视角下的形状。 Single-view Reconstruction 作者为了实现在重建的三维模型在符合单视角约束的前提下，对不可见视角进行合理猜想，设计了三个约束条件Front Constraint,Diversity Constraint,Latent Space Discriminator。 Front Constraint Front Constraint的作用是将收到视角约束限制的点从整个点云中分离开来。作者的做法是首先将重建得到的三维点云投影至二维平面以得到depth图片，然后再计算出哪些三维点构成了这些depth图片。文中将这些点的数量记为$N_i$ Diversity Constraint Diversity Constraint是限制剩下的$N-N_i$个生成点的位置。作者利用&nbsp;&nbsp;$loss_{div}=max(0,||r_1-r_2||_2 - \alpha EMD(S_1,S_2))$ 来约束两个由同一张单视角图片生成的点云之间的差异性。$r_1，r_2$表示两个不同的随机向量，作者用这个来使得生成的模型具有一定的，可控的差异性。 Latent Space Discriminator Latent Space Discriminator被作者用来使生成的若干个三维模型不仅要符合单视角图片的约束，还需要符合一定的合理性。作者首先利用已有的点云表征学习的方法，训练了一个auto-encoder。接着作者将auto-encoder中的解码器移植到他自己的网络结构最后面，并将生成的三维模型作为输入，生成三维模型。 $loss_{gan} = - \mathbb E_{I_i \thicksim p_{data},r_i \thicksim p(r)}[D(E_I(I_i,r_i))] + \mathbb E_{S \thicksim P_{data}}[D(E_S(S))] -\lambda \mathbb E_{\hat z \thicksim p_{\hat z}}[(||\triangledown_{\hat z}D(\hat z)||_2)^2] \tag{1}$ 其中$E_I$为作者自己网络的编码器，$E_S$为auto-encoder的编码器，$D$为判别器，$S$为从训练数据集中采样的三维模型。作者认为该结构可以学习到形状先验，以保证生成的三维模型的合理性 Synthesizing Multi-view Predictions有了上面三个约束条件，作者预训练了一个单视角生成网络，但是网络的最终目的是生成一个准确的三维模型，这就需要用到多视角约束。 根据上图可知，输入有单视角图片扩展为多视角图片，然后为每个输入图片生成一个三维模型。接着根据&nbsp;&nbsp; $loss_{consis} = \frac{2}{n(n-1)}\sum_{i=1}^{n-1}\sum^n_{j=i+1}CD(S_i,S_j)$&nbsp;&nbsp;来约束这几个三维模型的一致性。 根据算法步骤可以发现一个有意思的地方，作者称为heuristic search in initialization。作者首先随机选择5组不同和的${r_{ij}}^5_{j=1}$,每组包含的数量与输入图片数量一致。接着就分别把这5组$r_{ij}$结合同一组输入图片，输入到网络中，得到5个不同的$loss_{consis}$,把得到最小$loss_{consis}$的那组$r_{ij}$记为${r_i^+}{i=1}^n$。 接着在开始训练前，将预测模型Freeze,只根据loss值更新$r{ij}$，直到收敛。 Conclusion1、将多视角重建问题分成两个部分，第一部分进行单输入图重建，并创造性的对固有的模糊性进行建模。作者对此在文中解释道： However, different from the scenarios of generation in CGAN [27], we only have limited groundtruth (in fact, only one shape per image) which cannot span the reasonable shape space. We aim to learn a mapping to approximate the probabilistic model $p(S|I)$) in the reasonable shape space. 作者认为，单单靠一个监督是不足以使网络学习到一个可靠的形状空间。尽管作者做了消融实验证明了有效性，但是仍不足说服我。一个上百个点组成的三维空间，靠多预测个位数的三维模型，就能完成建模？） 2、作者采用的启发式初始化有一定的可取之处，让网络自身决定合适的初始参数是什么]]></content>
      <categories>
        <category>论文阅读</category>
        <category>刚性物体重建</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>模糊性建模</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(六)Three-D Safari Learning to Estimate Zebra Pose, Shape, and Texture From Images “In the Wild”]]></title>
    <url>%2F2019%2F08%2F23%2F8f9285bb%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;(六)Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images “In the Wild”论文链接 Github Contribution 提出了一个基于已有统计模型的，构造合成数据集的方法 针对四足动物有更复杂的形状问题，改进了已有纹理预测的方法 为了进一步对不同实例的改进，提出了per-instance optimization Method Net Architecture 输入图片经过Encoder(主要是ResNet-18)，输出为一个特征向量，接着送入三个分支网络。由于网络最后会生成一个一个full texture map，作者基于此对于每个实例做了进一步优化（per-instance optimization）。 digital dataset 这篇论文是基于作者之前的两篇工作SMAL和SMALR。其中前者是利用一组动物玩具扫描进行统计建模得到一个统计模型，也就是上图的SMAL horse template。之后在根据一系列优化函数进行给定输入图像的优化，三维模型可以建模为： $v = M(\beta,\theta,\gamma) \tag{1}$ $v_{shape}(\beta) = v_{horse} + B_s\beta \tag{2}$ 其中$\beta$为一组基$B_s$的系数，表示为动物形状，$\theta$为pose参数，$\gamma$为相机参数。后者较于前者，输入由单张图片变为对于同一个动物的（可以是）不同相机角度，不同姿态的若干张图片。作者首先利用SMAL对输入的多张图片进行拟合，得到对应的三维模型$v_i = M(\hat \beta_i,\hat \theta_i , \hat \gamma_i)$。由于不同照片所对应的是同一动物，所有用以描述形状的$\hat \beta_i$应该是一致的，所以可以任选一个。接着，作者对上面所获得的三维模型进行进一步的优化，以期获得待重建动物独有的形状特征，例如犀牛的角。 $v_{shape}(dv^{SMALR}) = v_{horse} + B_s \hat \beta + dv^{SMALR} \tag{3}$ 其中$dv^{SAMLR}$就可以看作待重建动物独有的形状特征。 作者利用有限的，真实的动物图片，通过SMALR合成了10个不同的三维模型。然后这个10个不同的模型，再加以不同的，合理的相机参数$\gamma$，pose参数$\theta$，形状参数$\beta$,然后投影在不同的二维背景上，以获得合成数据。这些数据用来作为网络的训练集，而测试集和验证集均为真实的图片。因此，对于每一组训练数据，都包含了：1、一张RGB图片，2、Texture map $T_{gt}$，3、texture uv-flow $uv_{gt}$4、剪影(sihouette) $\mathcal S_{gt}$，5 、pose参数 $\theta_{gt}$，6、相机参数 $\gamma_{gt}$，7、形状参数 $\beta_{gt}$，8、vertex displacements $dv_{gt}^{SMALR}$，9、特征点 $K_{2D,gt}$。为了更好的预测作者将9和7结合在一起得到10、$dv_{gt} = B_s\beta_{gt} + dv_{gt}^{SMALR}$ uv-flow predictor 大体思路与CMR一致，不同的是，作者认为，由于四足动物有着更大的articulated model,导致了在空间上具有不连续性（我估计可能是四肢之间具有较大空当的意思）。这就是个原本的纹理预测方法效果较差。作者的方法就是将其分成四个子块，同四个不同的编码，解码器来预测。但具体是如何做的，不清楚。 或者换种角度看待，这个问题.CMR 中初始模型是由一个单位球体得来的，这使得从中心到各个点的连线上不会存在第三个点。而对与这个工作中的统计模型，即使把点单位化，从中心到各个点的连线上也可能会存在第三个点，从而导致UV展开后的坐标一致，不利于网络预测。 shape prediction $dv = Wf_s + b \tag{4}$ 其中$b$是偏置项，$W$被初始化为SMAL的$B_s$，为了获得更好的三维模型，$B_s$会随着迭代进行优化。 $L_{train} = L_{mask}(S_{gt},S) + L_{kp2D}(K_{2D,gt},K_{2D}) + L_{cam}(f_{gt},f)+L_{img}(I{input},I,S_{gt})+L_{pose}(\theta_{gt},\theta)+L_{trans}(\gamma_{gt},\gamma)+L_{shape}(dv_{gt},dv)+L_{uv}(uv_{gt},uv)+L_{tex}(T_{gt},T)+L_{dt}(uv,S_{gt}) \tag{6}$ per-instance optimization Given an input image, we run the regression network and then perform a per-instance optimization where we keep the network layers fixed and optimize over the feature space variables. In this way we eploit the correlation between variables learned by the network. 由于不是很确定这个方法是如何对特征空间进行优化的和优化的先后顺序是怎样的，所以就不下结论了，等开源之后，再补回来。这里就记录一下关于这个方法的一些可能的意义。 网络的输入为input image,将其输出的三维模型投影回二维平面的图片记为prediction(与网络结构图一致)。作者的意思就是将prediction加上具有一定意义的background之后（记为back-prediction），再跟input image进行比较。其中优化函数就包括了perceptual loss。 per-instance optimization有何用 由于Encoder所编码的features包含了很多的噪声，而这些噪声会干扰接下来三个分支网络的结果预测。如果去除这些噪声，毫无疑问，会有利于参数的预测。我们可以将一幅图片分为前景和背景两个部分,从而可以将perceptual loss的评价方法分为前景损失和背景损失两种。在per-instance optimization中，由于网络所有层的参数都是固定的，导致了back-prediction和input image的背景都是一致的，那么为了降低损失，网络就势必要尽可能提高两者在前景上的相似度，从而就去除了features中的噪音。 为什么要加background model 文中说的很清楚，如果在不加background model，那么在测试时，就要求输入图片一定具有所对应的剪影，否者单单有prediction是没法跟input image进行peceptual loss上的比较。而background model,如文中所提，就是预测一个平均的背景像素值，来代替真实的背景值。文中提到这个模型使用构造SMALR模型所用的图片来训练的 为什么需要预测背景的平均值 尽管这是一个说服力很弱的猜想，但是记录一下。 横轴可以看作难易度，纵轴可以看作背景的相似度。back-model的作用就可以看作为左侧橙色的线，而要预测剪影的话，就可以看作右侧黄色的线。尽管back-model的效果较弱，但其所需的成本较剪影预测要少很多，同时获得的效果也可以达到预期的效果。 Experiments]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维动物重建</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>SMAL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(五)Learning View Priors for Single-View 3D Reconstruction]]></title>
    <url>%2F2019%2F08%2F19%2F54bbc8cd%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Learning View Priors for Single-view 3D Reconstruction论文链接 Contribution 提出了新的视角先验方法，该方法弥补了对称先验的不足，虽然重建物体大多是符合对称先验的，但是在拍摄照片的那一瞬间，由于拍摄角度和物体本身发生的一些形变等原因，使得待重建的物体本身并不是对称，这样便会使得重建效果不佳。 提出了internal pressure,即点可在与面片垂直的外方向尽可能膨胀。使得重建的物体更加饱满，看起来更加真实。 Methond 本文主要介绍其中的单图重建工作，方法可以很好的泛化为多图重建方法。 View prior 其核心Loss函数为： $\mathcal L_{r}(x,v) = \sum_{i=1}^{N_0} \mathcal L_{v}(P(R(x_{i1}),v_{i1}),x_{i1}) \tag{1}$其中$R\left (* \right )$表示为重建函数，$P \left (* \right )$表示将重建好的三维物体按照$v_{i1}$视角投影会二维平面。而$\mathcal L_{v}$为衡量$x_{i1}$和$v_{i1}$视角下的重投影结果之间的误差（a function that measures the difference between two views）。 $\mathcal L_{s_1}(x_s,\hat x_s) = \sum_{i=1}^{N_s}\left( 1- \frac{x^i_s \cdot \hat x^i_s}{|x^i_s||\hat x^i_s|}\right) \tag{2}$ $\mathcal L_{s_2}(x_s,\hat x_s) = 1 - \frac{|x_s \bigodot \hat x_s|_1}{|x_s+\hat x_s + x_s \bigodot \hat x_s|_1} \tag{3}$ $\mathcal L_v = \mathcal L_s + \lambda_c \mathcal L_c \tag{4}$ 其中$\mathcal L_c$为perceptual loss输入为RGB图片。$\mathcal L_s$的输入为剪影。$\mathcal L_{s_1}$是计算两个mask之间的余弦距离（可能是代表相似度？？），作者使用了多尺度mask,$N_s$代表下采样的次数。 $\mathcal L_{s_2}$为IOU（intersection over union)函数，为的是约束其剪影尽可能重叠。正如上面的$\mathcal L_r(x,v)$所介绍的那也，以上的所有函数都是为了评价两张图片($x_s,\hat x_s$)之间的相似度,其中前者为ground-truth图片,后者为预测的图片。在此基础上，作者使用GAN网络来学习训练数据集中待重建物体不同视角的先验知识,而上面这个Loss函数是用来限制生成器的，根据原图和重投影图片之间的误差不同，来更新生成器的参数，以期获得更好的重建效果。 可以看到，对于椅子，使用作者提出视角先验的方法，重建的结果从各个视角看上去都更加符合实际情况。 基于这种情况，作者使用判别器来给生成器提供不同视角的重建信息。 $\mathcal L_d(x_{ij},v_{ij}) = -log(Dis(P(R(x_{ij}),v_{ij}),v_{ij}))-\sum_{v_u \in \mathcal V ,, v_u \neq v_{ij}} \frac{ log(1- Dis(P(R(x_{ij}),v_{u}),v_{u}))}{|\mathcal V -1|} \tag{5}$ 整个网络的输入为单张RGB的图片，生成器首先依据图片重建出一个对应的三维模型，再根据这张图片所对应的相机视角反投影回二维平面，获得的图片可以记为$x_{gt}$。接着，在从$\mathcal V$中选取任一与之前的相机视角不一致的的相机视角，并依据此相机视角，再一次投影会二维平面。这是，第一次投影所得的$x_{gt}$就可以当做真实数据，第二次投影所得就可以当做加数据，一同送入判别器进行判断。作者在文中提到，所有的视角都是存在于训练集中的（$\mathcal V$ be the set of all viewpoints in the training dataset）。这是否意味着对于一些在训练集中不存在的视角，该网络还是难以学习到其形状？ Internal Pressure 这是一个较弱的先验知识，这个是受启发与Visual hull重建技术。通俗的来讲，就是使重建结果，在不改变投影误差的限制下，尽可能的朝着某个方向膨胀。作者采用的方法就是对每个点施加了一个沿着所在面片法向量的梯度（Concretely, we add a gradient along the normal of the face for each vertex of a triangle face. Let p i be one of the vertices of a triangle face, and n be the normal of the face）。为了实现这一效果，需要添加一个函数，该函数需要满足:$\frac{\partial \mathcal L_{p}(p_i)}{\partial p_i} = -n \tag{6}$ 即关于$p_i$点的偏导等于该点所在的面片的法向量$\overrightarrow n$。文中没有提及具体的函数选择。 Experiments 作者在文中提到，对于手机，沙发，飞机等物体。由于其形状单一，获取的多视角先验可以很好的解决在不可见视角下，重建效果差的问题。但是对于灯(lamp）这类物体，由于其形状的多样性，现有的网络并不能很好解决这个问题。]]></content>
      <categories>
        <category>论文阅读</category>
        <category>刚性物体重建</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>视角先验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(四)Canonical Surface Mapping via Geometric Cycle Consistency]]></title>
    <url>%2F2019%2F08%2F09%2F6021dd11%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Canonical Surface Mapping via Geometric Cycle Consistency 论文链接 Github链接 Contribution 利用三维Template做为中介，弱化了实现稠密的，准确的多张图片对其所需的监督条件。 已有的图片到图片，图片到三维模型的映射关系的形成工作，不外乎依赖于人工手动标记对应点或者按照某些形变规则，将形变施加于图片A获得图片B,从而获得了A,B之间的对应关系。这些方法对于光照，角度等变量的鲁棒性较弱。 Method Preliminaries 作者利用两个参数将三维template平面化，$\overrightarrow u(u,v)$ 其中$u \in (0,1) v \in (0,1)$ 并且定义$ \phi(\overrightarrow u) $为恢复三维$(x,y,z)$的操作。作者定义$\mathcal C \equiv f_\theta(I)$ 其中$f_\theta(I)$表示网络参数，作者将网络的预测行为抽象定义为$\mathcal C$ Geometric Cycle Consistency Loss $ \mathcal L_{cyc} = \sum_{p\in I_f} ||\overline p - p ||_{2}^{2} \quad;\quad \overline p = \pi(\phi(\mathcal {C}[p])) \tag{1}$ 上面这个就是作者提出的一致性Loss,很好理解，就是像素$p$先经过网络映射在三维template上，接着投影回原图，两者之间的距离要尽可能小 Incorporating Visibility Constraints 由于相机视角的原因，任何物体都会存在一个自遮挡的问题，而这个问题在预测图片稠密映射时就会产生一定的干扰。比方说，从正面看过去，一只鸟类的喙很有可能与其尾巴处于一条线上，所以当投影至二维像平面式，尾巴上某点就会被喙给遮挡。在这种情况下，如果映射关系预测网络将图片上的喙映射在三维template的尾巴上，最后投影回像平面计算$\mathcal L_{cyc} $的时候，依旧Loss会很小。所以作者就提出了： $\mathcal L_{vis} = \sum_{p \in I_f}max(0,z_p - D_{\pi}[\overline p]) \tag{2}$ 其中$D_{\pi}[\overline p]$是三维template在对应相机参数下的深度图。 Mask Re-projection Loss &amp; Multi-Hypothesis Pose Prediction 作者为了彻底摆脱对基准相机参数的需求，又新增了一个网络$g_{\theta^{‘}}$ 这个网络是用来预测相机，并且为了避免局部最小解(local minima)，作者利用预测多个相机参数来达到这个目的。最后，作者使用一个约束来指导相机参数$\pi$的预测 $ \mathcal L_{mask} = ||f_{render}(S,\pi) - I_f||^2 \tag{3}$ 其中$f_{render}$是在预测相机参数下，三维template投影至二维的Mask。 正如上面提到，作者会一次性预测多个相机参数，所以有${ (\pi_i,c_i)} \equiv g_{\theta’}(I) \quad i=1..8 $,其中$c_{i}$表示每个预测结果的概率是多少 。 最后，总的约束函数为：$\mathcal L_{tot} = \mathcal L_{div}(g_{\theta’}(I)) + \sum_{i=1}^{N_c}c_i(\mathcal L_{cyc}^i + \mathcal L_{vis}^i + \mathcal L_{mask}^i) \tag{4}$ 网络结构如图所示，对于2D-3D的映射关系预测，作者是利用一个U-Net结构（红色标注），输出为$B*4*H*W$的特征图，其中[:,:3,:,:]表示的是预测的三维坐标。[:,3,:,:]表示的是预测的mask。$H,W$分布代表输入图像的长和宽。$g_{\theta’}$表示相机参数预测网络，具体上，作者首先用ResNet18提取图片特征，接着送入FC层预测相关相机参数。作者在相机参数的预测网络上，有很多设定还是十分有趣的，有兴趣可以找来看下。 大致映射 之所以论文里提到approximate这个词呢，是因为，从代码上来看，$f_{\theta’}$的输出$BHW*4$中关于2D pixel to 3D vertex的映射并不一定是在template mesh上，对于网络输出的一对UV坐标(u,v)，作者首先是找到离这对坐标最近的面片是哪一个，接着计算该点关于这个面片的重心坐标(barycentric coordinate),最后根据重心坐标和面片三个顶点的位置坐标，得到最终的3D坐标，同时也确保了这个坐标在template mesh上。可以想象一个四面体（A-BCD），网络预测的3D坐标可能是A点，那么作者就是找到A点在BCD上的对应点。 Experiments 咋一看结果挺好，但是不知道作者为什么没有把CMR带基准相机参数的评测结果放出来，从消融实验的结果来看，在不利用预测相机参数的情况下，CSM结果并没有比不使用预测相机参数的CMR好太多，甚至在cars类别上的评分还低些]]></content>
      <categories>
        <category>论文阅读</category>
        <category>Surface Mapping</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(三)Paperset:how to Extract Geometric Features From Spatial Layout of Points]]></title>
    <url>%2F2019%2F06%2F06%2Fa24b1b46%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Paper set: how to extract geometric features from spatial layout of points [1]TOG2018 Dynamic Graph CNN for Learning on Point Clouds [2]CVPR2018Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling [3]AAAI2019MeshNet: Mesh Neural Network for 3D Shape Representation [4]CVPR2019Structural Relational Reasoning of Point Clouds [5]CVPR2019Relation-Shape Convolutional Neural Network for Point Cloud Analysis Dynamic Graph CNN for Learning on Point CloudsDescription:作者认为Pointnet和Pointnet++均只是探究了点云局部的独立的，本身的几何结构（Pointnet尽管是将一个完整点云分割为若干子点云来分别计算几何结构，但其还是处于local sacle的程度），并没有得到自己与其他点，其他子点云之间的关系。作者利用图神经网络在处理不规则数据上的优势，在抽取点云中各个点之间的几何关系，从而弥补了之前方法的不足 Contribution:作者提出了一种可以在点云处理中使用的操作：EdgeConv。并且通过大量的实验证明了改操作可以更好的在提取点云局部几何特征的同时保持排列不变性。 Method:作者在relate work-&gt;Geometric Deep learning中提到A key difficulty,however,is that the Laplacian eigenbasis is domain-dependent;thus, a filter learned on one shape may not generalize to others. 就是对于图卷积网络来说，基于频域的卷积其泛化性不如在空域中的卷积。 EdgeConv 假设点云点数为N,同时每个点上的特征维度为F,那么就有$X = {x_{1},…,x_{n}} \subseteq R^{F}$ 。对于每个点上的特征向量的计算，选取在特征空间上与中心点最近的K近邻点(k-nearest neighbor) 表示为$\mathcal G = (\mathcal V, \varepsilon)$ 其中$\mathcal V={1,…,n }$代表点，$\varepsilon$表示中心点与其K近邻的边。那么EdgeConv的公式可以表示为： $ x_{i}^{‘} = \square h_{\theta} (x_{i},x_{j})$ &emsp;&emsp;$s.t. j:(i,j) \in \varepsilon$ $x_{i}$表示在i th点上的特征向量 其中$\square$ 代表channel-wise的对称aggregation操作，例如叠加或求最大值（to keep permutaion invariance)，每层EdgeConv对每个点$x_{i}$都会更新其特征向量本文采用的Max Aggregation方法 Choice of $h_{\theta}$ $h_{\theta}(x_{i},x_{j}) = \theta_{j}x_{j}$ 这就相当于普通的CNN卷积了，$x_{i}$就相当于卷积核中间的那个点，$x_{j}$是其周围的点，$\theta$是可学习参数 $h_{\theta}(x_{i},x_{j}) =h_{\theta}(x_{i})$ 这就是Pointnet的处理方式，只独立的考虑一个点 $h_{\theta}(x_{i},x_{j}) =h_{\theta}(x_{i}-x_{j})$ 该方法作者认为只考虑了编码局部信息，失去了全局形状结构 $h_{\theta}(x_{i},x_{j}) =h_{\theta}(x_{i},x_{i}-x_{j})$ 弥补了上一个方法的不足，也是本文所采取的方法 Dynamic graph update 在每一层EdgeConv之后，作者都会计算一个根据新得到的每个点的特征向量，计算一次KNN,并用这个新的KNN Graph来计算下一层的特征向量。 Implementation 注意EdgeConv层的维度 Experiment 首先当然是取得了state-of-art的成绩。有趣的，作者发现对于近邻K的选取，不是越大越好，对于不同的任务，都有一个峰值K使得效果最好，作者认为过大的K会使得欧式距离不能很好的估计测地距离（geodesic distace)会摧毁几何信息。同时作者发现K的选取跟点的密度有关 Mining Point Cloud Local Structures by Kernel Correlation and Graph PoolingDescription:这篇文章与[1]是同一时期的论文，但是在ModelNet40上的分类和ShapeNet上的分割都略微不如[1],但其思想是值得借鉴的。 作者利用在04年ECCV上的一篇关于point cloud registration工作上的技术kernel Correlation来对局部几何信息进行编码，可以理解为2D CNN中卷积核(kernel)的一种特殊的3D变种，其会对具有不同结构信息的点产生不同程度的响应(response)。同时作者也提出了一种利用K nearest neighbor graphs(KNNG)的新的特征聚合方法（feature aggregation)来利用高维结构特征推断集合信息。 Contribution:作者提出了借用kernel correlation的思想来简化求解local structural feature Method: Local Geometric Properties Surface Normal: 作者在论文中提到可以用PCA求解的minimum variance direction作为局部的法向量，原话是:surface normal can be estimated by principal component analysis on data covariance matrix of neighboring points as the minimum variance direction Covariance Matrix(协方差矩阵): 作为二阶局部描述子，它能提供比normal更加多的信息量。但是作者认为normal与Covariance Matrix作为手工设定的特征，其描述局部结构特征的能力有限，因为不同形状的几何结构可能产生相似的normal或者covariance matrix，这就给后续工作带来巨大的噪音。 Kernel Correlation: Kernel Correlation可以帮助用来判断两个Point cloud之间的相似度（similarity)。在point cloud registraion当中，一个point cloud作为reference,另一个作为source。source要不断的进行刚性/非刚性变换以使得kernel correlation的响应达到最大。也就是说，响应大的时候，两者的几何结构是非常相似的，那么这种性质就可以用来构造卷积核。 Learning on Local Geometric Structure point cloud registration 中的kernel corelation是固定一个template(reference),试图找到一个变换（transformation)使得source+transformation的结果与refrence的响应最大。而本文作者对此进行了改进，在整个训练过程中，template(reference）不再被固定，而是通过反传误差，不断学习到一个合适的reference(这里可以看成不断调整初始reference中点的位置），从而使reference与source的响应达到一个水准，这个水准能与网络其余结构一起产生预期的预测效果。论文原话是： Under this setting, the learning process can be viewed as finding a set of reference/template points encoding the most effective and useful local geometric structures that lead to the best learning performance jointly with other parameters in the network. kernel corelation的计算公式： $KC(\mathcal k,x_{i} ) = \cfrac{1}{\mathcal N(i)}\sum\limits_{m=1}^{M}\sum\limits_{n \in \mathcal N(i)}K_{\sigma}(\mathcal k_{m},x_{n}-x_{i}) \tag {1} $ $\mathcal k_{m}$表示kernel中的m个可学习的点，$\mathcal N(i)$ 表示anchor point $x_{i}$的周围K近邻，而$K_{\sigma}(*,*)$表示核函数，也就是公式$(2)$所示。公式$(1)$大体就是对于某个corelation kernel来说，其中每个点$\mathcal k_{m}$都会与$\mathcal N(i)$中包括的点一起作为公式$(1)$的输入，也就是会计算m*k次。$K_{\sigma}(k,\delta) = exp(-\cfrac{||k-\delta||^2}{2\delta^2}) \tag {2}$ 分子表示欧式距离，分母$\delta$表示核的宽度，它控制点之间距离的影响（这里我也不是太懂）。文中的$\delta$被设置为the average neighbor distance in the neighborhood graphs over all training point clouds。 可以设置$L$个kernel corelation,这个$L$与卷积网络中的output channel类似。 Learning on Local Feature Structure 核心思想就是每个点预期近邻点的几何结构是一致，将各个点与其自身的近邻点做特征向量的aggregation有利于使得网络具有更好的效果和鲁棒性。作者采用了average pooling和max pooling， 其中max pooling为 $\mathcal Y(i,k) = \max\limits_{n \in \mathcal N(i)} X(n,k) \tag{3}$ k表示第i个点的特征向量的第k个维度，特征向量维度与kernel corelation的核数量一致 average pooling为：$Y=D^{-1}WX \tag{4}$$W$为邻接矩阵，$D$为度矩阵(degree matrix) 本文采用的是Max pooling作为Aggregation方法 Experiment: 作者通过Ablation study从Effectiveness of Kernel Correlation,Symmetric Functions,Effectiveness of Local Structures三个方面证明了本文所提方法的有效性： 但我觉得存在一个问题:在Effectiveness of Kernel Correlation试验中，每个点的Normal是使用PCA方法求出的，然后再与坐标Catenate.那么所求的Normal与真实的值相差多少呢?这个误差是否会影响实验结果呢？如果改用更加准确的Normal求解方法，是否能提高，甚至超过Kernel Correlation方法呢？ Conclusion:这篇工作的根本目的在于探究一种较PointNet++更为简单的局部几何计算子，从而在节省算力的同时，还能提高准确度。但是其本质上是以一种更加简单的方式回答what type of the point，其没有解决点与点之间是什么关系，换句话说，PointNet++与KC-Net的本质与[1]中的DG-CNN是不同的，这可能也是造成他们效果差异的原因。 MeshNet: Mesh Neural Network for 3D Shape RepresentationDescription: 本文提出一种基于Mesh的3D shape特征抽取方法,直接将对于Mesh-based 3D shape的分类准确率从68.3%提升到91.9%。作者将单独的一个Face视为一个Unit,这样做可以消除Mesh固有的irregular，因为每一个face有且只有3个面与其相邻（作者这里用的是no more than，我认为不对)。其次，作者将一个Face的Feature分为spatial feature和structural feature,这样做有利于是Face Feature具有更加明确的指向性，使网络更容易学习到有用的Feature Method: Input: 作者将Face-unit进行了如下分解： Face Information: Center:coordinate of center point Corner:vectors from the center point to three vertices Normal: the unit normal vector Neighbor Information: Neighbor Index: indexes of the connected faces(filled with the index of itself if the face connects with less than three faces) 如果面片形成的是一个闭集，那么必然是一个面片有三个相邻面的 Spatial and Structural Descriptor spatial descriptor: 输入只是Center坐标，简单的经过MLP得到结果 Structural descriptor:face rotate convolution $g(\cfrac{1}{3}(f(v_{1},v_{2})+f(v_{2},v_{3})+f(v_{3},v_{1})) \tag{1}$ $f(*,*) : \mathbb R^3 * \mathbb R^3 \rightarrow \mathbb R^{K_1}$ $g(*,*) : \mathbb K^1 \rightarrow \mathbb K^2$可以是任何合法函数（大多都是MLP) 作者将这个操作类比到卷积操作，$f(*,*)$看作卷积核，每次对两个向量做卷积，然后用旋转代替平移，步长为1。$K_1$表示output channel。这个$\cfrac{1}{3}$可以当做对称函数(average pooling)用来消除disorder，保证了permutation invariance。 Structural descriptor: face kernel correlation: 作者利用[2]的kernel corelation将其改为针对face-unit的形式，利用面片的normal代替[2]中点的坐标。由于所有的normal都是单位向量所以作者用$(\theta,\phi)$ 来代替单位向量$(x,y,z)$ 所以有 $\begin{cases} &amp; x=sin \theta cos \phi \\ &amp; y=sin \theta sin \phi \\ &amp; z=cos \phi \tag{2} \\\end{cases}$ 其中：$\theta \in [0,\pi], \phi \in [0,2\pi)$(其实就是仰角和旋转角) $KC(\mathcal k,x_{i} ) = \cfrac{1}{\mathcal |N(i)| \mathcal |M_k|)}\sum\limits_{n \in \mathcal N_i}\sum\limits_{m \in \mathcal M_k}K_{\sigma}(n,m) \tag {3} $ $K_{\sigma}(n,m) = exp(-\cfrac{||n-m||^2}{2\delta^2}) \tag {4}$ 这些都与[2]中的含义一致 Mesh Convolution 如图所示，作者将spatial Feature，structral Feature和Neighbor Index，作为输入送进Combination和Aggregation中，得到新的spatial Feature和structral Feature。 作者在文中提到，之所以不将spatial Feature一起送进Aggregation是因为不想让structral Feature受到点所在空间信息的影响，就如2维卷积的过程中，不会显示的告知卷积核该像素的绝对坐标一样。 图中标识了三种Aggregation方法：Concatenation,Max pooling和Average pooling 作者认为Average pooling是很多强响应被忽略了，从而损失了很多的有用信息。实验表明Concatenation效果最好。 Conclusion: 将Feature拆分为多个更加明确的子特征，可能有助于网络学习 本文有个缺点就是没有考虑显示的考虑面片之间的结构关系，每个面片的特征都只与其本身有关，但不排除在之后了MLP层之后学习到了。 结果显示在移除了MeshConv之后，准确度下降的比重不是最大的 Structural Relational Reasoning of Point CloudsDescription作者首先阐述了Pointnet++的缺点，与[1]中提到的一致–只是探究了局部的结构特征，没有探究这些局部之间的结构也就是依赖关系。但是与[1]不同的是，本文所提出的structural relation network(SRN)探究的是局部与局部的依赖关系(structuraldependencies of local regions)，而[1]是以点为单位的。作者以人类识别物体为例，认为对物体局部之间关系的认知对于理解物体本身具有很大作用 Method $P_i$代表子点云,$u_i \in \mathbb R^d$代表该子点云的局部几何特征，$v_i \in \mathbb R^3$表示该子点云的平均位置用来表示位置信息。作者认为$u_i$对发现重复的局部模式(repetitive local patterns)有帮助，而$u_i$对发现各个局部之间的连接关系(linkage relations)起到关键作用。所有$P_i$的结构关系特征表示为： $y_i = f(\sum \limits_{\forall j}h(g_u(u_i,u_j),g_v(v_i,v_j)) \tag{1}$ $j$表示除了$i$以外的所有子点云。在实现方面，$g_u(·，·),g_v(·，·)$都是由多层感知机(MLP)实现,$f(·，·),h(·，·)$都是由1 $\times$ 1卷积实现。由上面那张图可以看出每个$u_i$都会与其他的所有$u_j$做一次concatenating操作（应该是在channel维度进行拼接） 注意到标黄部分，采用了不同比例的局部特征,作者认为SRN对于不同的子点云获取方法都是鲁棒的。同时红框表示文中提到的residual connection$y$是作为对$u$的补充(complement) Experiment为了证明SRN对3D Shape的局部关系学习具有很好的泛化性，作者在ModelNet40（3D objects)和Scannet(indoor scenes)上做了跨库实验 作者提取分布在两个库上训练的网络所产生的特征向量(feature)之后用线性SVM做分类 作者为了进一步阐述SRN对特征提取的帮助，利用t-SNE(一种对高维数据降维可视化的方法）将所提取的特征进行可视化。发现大多数类别的类间差异都很小。 Relation-Shape Convolutional Neural Network for Point Cloud AnalysisDescription:作者依旧是从探究点与点之间的关系来入手，作者认为从点学习特征具有三点挑战： 需要具备排列不变性(permutation invariant) 对于刚性形变鲁棒，也就是即使点的绝对位置发生改变，只要其结构未变，其特征应该是一致的 效果好 Method $x_i$表示采样点,$x_j \in \mathcal N(x_i)$表示周围邻域点，则该$x_i$上的特征向量$f_{P_{sub}}$可以用来表示这个邻域的结构 $f_{P_{sub}} = \sigma(\mathcal A(\lbrace \tau(f_{x_j}), \forall x_j \rbrace)), d_{ij} &lt;r \forall x_j \in \mathcal N(x_i) \tag{1}$ $\tau(f_{x_j}) = w_{ij} · f_{x_j} = \mathcal M(h_{ij} · f_{x_j}) \tag{2}$ $d_{ij}$是$x_i,x_j$之间的欧式距离。对于上面这个公式，论文的解释是:Here $f_{P_{sub}}$ is obtained by first transforming the features of all the points in $\mathcal N(i)$ with function $\tau$ , and then aggregating them with function $\mathcal A$ followed by a nonlinear activator $\sigma$. 公式(1)相对于论文[1]多了一个$\tau$函数，其余操作一致，$\mathcal A$也是max pooling $\mathcal M$为三层MLP作者之后在试验中证明了3层是较好的选择 Conclusion这篇文章是被录用为CVPR2019的oral.但是其最核心的思想我认为跟[1]是差不多的，作者在文中也用一句话评价了两者的区别DGCNN captures similar local shapes by learning point relation in a high-dimensional feature space, yet this relation could be unreliable in some cases. 我刚看到简直吐血，真·不要碧莲。我认为之所以能被接受为oral原因如下： 优秀的写作，论文从问题到解决办法的提出一气呵成，非常漂亮，很多地方的写作技巧都很微妙，比如上面这句。 实验做得很充分，几乎论证了所有文中提出的tricks。同时效果确实有不小的提升。 具体用到的tricks如下： 利用了hierarchical architecture,在相同的权重下，对不同数量的neighborhoods进行学习，这与[4]是一致的，但文中没有提出这点 $\tau$函数的提出，新学习到的特征向量与老的特征向量做一次点乘,这与ICML2019GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects类似。 相对[1]对每个点求其邻域的特征向量，本文对每个选取的领域求特征 论文采取的对邻域的选取方法也与其他不相同可以看到，在scale=1的情况下，用K-NN选取邻域只有90.5%的准确率，而转为random-PIB之后就提升到了92.2%，这与DGCNN对每个点都求一个结构特征所得到的准确率是一致，但计算量却大大减少了 很可惜，对于核心方法的修改，作者没有给出消融实验的结果，不能看出新增的$\tau$函数对效果的提升有什么帮助。同时对于输入h是否会变也没有给出明确的说明，就符号一致性来说，是不会改变的,[1]中是会改变的]]></content>
      <categories>
        <category>论文阅读</category>
        <category>特征抽取</category>
      </categories>
      <tags>
        <tag>点云</tag>
        <tag>网格</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu18.04装机]]></title>
    <url>%2F2019%2F03%2F24%2F30a4c30f%2F</url>
    <content type="text"><![CDATA[MatlabR2016b 前期准备： R2016b_glnxa64_dvd1.iso，R2016b_glnxa64_dvd2.iso，Matlab 2016b Linux64 Crack.rar 具体流程： 1.挂在ISO文件，尽量挂在空间大点的盘上,并开始安装 12345mkdir matlabISOsudo mount -t auto -o loop R2016b_glnxa64_dvd1.iso matlabISO/这里不用加sudo, 如果加了，会使安装的matlab运行时只能让root用户使用，并且新建的文件普通用户也没有访问权限同时网上有说不能进入matlabISO里去执行installmatlabISO/install 2.选择使用文件安装密钥，同时可以Matlab 2016b Linux64 Crack.rar中获得安装密钥，接着就是默认安装 3.安装到80%左右时，会提示挂载dvd2镜像 12注意这里路径别写错，同时要挂载到同一个文件目录下sudo mount -t auto -o loop R2016b_glnxa64_dvd2.iso matlabISO/ 4.激活Matlab 1234567对文件赋予权限sudo chmod -R 777 llicense_standalone.liccd your/matlab/path/binsh matlab这时会弹出对话框，选择不联网方式激活，并且加载Matlab 2016b Linux64 Crack.rar中的license_standalone.lic将Matlab 2016b Linux64 Crack.rar/glnxa64/bin/glnxa64中的四个文件复制到Matlab中对应位置sudo cp lib* your/matlab/path/bin/glnxa64/ 5.设置matlab快捷方式 sudo gedit /usr/share/application Matlab.desktop 在打开的Matlab.desktop中输入一下内容 [Desktop Entry] Encoding=UTF-8 Name=Matlab 2016b Comment=MATLAB Exec=/your/matlab/path/bin/matlab Icon=/your/matlab/path/toolbox/shared/dastudio/resources/MatlabIcon.png Terminal=true StartupNotify=true Type=Application Categories=Application; Pycharm 前期准备 pycharm-professional-2018.3.5.tar.gz 安装流程 1.放到合适的目录，提取压缩包 2.进入你的压缩路径，运行安装脚本 123cd your/unpack/path/pycharm/bin注意不要加sudosh ./pycharm.sh 3.激活码激活 4.快捷图片 与Matlab操作一致 sougou拼音 前期准备 sogoupinyin_2.2.0.0108_amd64.deb 安装流程 1.安装fcitx 12sudo apt-get install fcitx-binsudo apt-get install fcitx-table 2.配置fcitx 搜索语言支持，将键盘输入法系统设置为fcitx 3.安装sogoupinyin_2.2.0.0108_amd64.deb,并重启电脑 4.点击Ubuntu右上角的键盘，点击设置，将出入搜狗拼音其余输入法删除 cuda 安装流程 1.安装所需依赖项 123sudo apt-get install freeglut3-dev build-essential sudo apt-get install libx11-dev libxmu-dev libxi-dev sudo apt-get install libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev 2.安装CUDA 12345678910111213141516#进入cuda目录cd brl/cuda#安装cudasudo sh cuda_9.0.176_384.81_linux.run --override#当你输入上面这条命令时，终端界面会让你输入一些选择，需要注意的有下面两点：# 1、是否要安装显卡驱动--选择否# 2、决定安装位置--根据自身情况选择安装位置（下面四个补丁也要安装在这个位置上）# 3、除了1以外所有的选择，全部选是（Y）#安装cuda的补丁1sudo sh cuda_9.0.176.1_linux.run#安装cuda的补丁2sudo sh cuda_9.0.176.2_linux.run#安装cuda的补丁3sudo sh cuda_9.0.176.3_linux.run#安装cuda的补丁4sudo sh cuda_9.0.176.4_linux.run 安装完整之后因为没有安装它提示的驱动，所以会提示安装不完整 3.添加环境变量 这里需要注意，当你安装CUDA时，会问你是否更新（创建）软连接，若你是第一次安装CUDA或者你想代替你当前使用的CUDA版本，就要选Y。安装程序就会帮你创建/usr/local/cuda/bin链接到你的CUDA安装位置，若电脑中存在多个CUDA版本时，理论上只要删除这个软连接，再新建一个指向你想使用的CUDA上即可 123456sudo gedit ~/.bashrc在打开的文件中输入export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;另开终端输入source ~/.bashrc cudnncudnn就相当于一个插件，可以随时更换，只要删除老的Cudnn文件即可 安装流程 1.解压cuDnn 2.复制其中文件到你想要使用CUDA中 12345#这里就是cuda,因为你解压缩以后的文件名就是cudasudo cp cuda/include/cudnn.h /usr/local/cuda/include/ #*****sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -dsudo chmod a+r /usr/local/cuda/include/cudnn.hsudo chmod a+r /usr/local/cuda/lib64/libcudnn* 3.进入CUDA更新软连接 123456789由于/usr/local/cuda与你使用的CUDA建立了软连接，所以当你更新这里面的文件时，其他位置的CUDA也会进行相应更新cd /usr/local/cuda/lib64/sudo chmod +r libcudnn.so.7.3.0sudo ln -sf libcudnn.so.7.3.0 libcudnn.so.7sudo ln -sf libcudnn.so.7 libcudnn.sosudo ldconfigsudo ldconfig /usr/local/cuda/lib64/根据cudnn的版本不同libcudnn.so.7.3.0中的数字部分需要修改]]></content>
      <categories>
        <category>常用</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(二)End-to-End Recovery of Human Shape and Pose]]></title>
    <url>%2F2019%2F03%2F09%2Fe6413cd%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;End-to-end Recovery of Human Shape and Pose论文链接 Github链接 Contribution 单图重建 利用SMPL进行重建，并且达到real-time效果 不要求每一个训练的image都要有其对应的3D ground truth Method 作者提出了一个end-to-end的网络来从单张RGB人像恢复其3D形状。作者利用已有的SMPL模型，SMPL模型可由3D relative joint rotation和Shape 来刻画一个人的3D shape。作者认为在以往的人体建模工作当中，有3D joint location来估计一个完整的3D shape 是不鲁棒的。原因有二：1、3D joint location alone do not constrain the full DoF at each joint. DoF意思就是景深，这个跟相机的参数有关。对这个参数的估计错误，可能会导致所估计的3D shape在图像中的显示位置有区别（我自己想的，不一定正确）2、Joints are sparse, whereas the human body is defined by a surface in 3D space. 这个就是Joints的点数过少，不足以约束一个完整的3D human shape。 作者还解决了两个在重建工作的问题。其一就是缺少3D ground-truth数据，同时，已有的3D ground-truth绝大部分是在实验室环境下采集的，其对in the wild的2D图像泛化性很差。其二就是单视角情况下2D-3D mapping的问题，不同的3D shape存在对应一张相同的2D图片，在这些3D shape中，存在一些不正常的shape(may not be anthropometrically reasonable) 网络训练 网络结构 Encoder和Regression 网络首先输入一张图片经过Encoder(Resnet-50)后，输出一个feature$\in \mathbb{R}^{2048}$。之后输入到一个Regressor（3层FC,2048D-&gt;1024D-&gt;1024D-&gt;85D）进行迭代，总共会迭代3次，每次迭代过程中会输入一个cam_para,shape $\beta$,pose $\theta$。将pose和shape输入smpl可以得到本次迭代后预测的模型（包括3D joint location）。由cam_para和3D joint location可以得到2D joint location从而可以计算$L_{reproj}$。由pose(也就是joint rotation)和shape可以计算出$L_{adv}$。若输入的2D image有对应的3D数据，则还可以计算$L_{3D}$。 但是在最后计算loss的时候，只会使用每次迭代时产生的$L_{adv}$，而$L_{3D}$和$L_{reproj}$只会利用最后一次迭代产生的loss。这是由于作者认为若每次迭代产生的loss全部都利用,这容易导致regressor限于局部最优化。 Adversarial Prior Adversarial Prior是用来解决上面提到的生成3D数据不真实情况的一个判别器。由于该判别器的任务是判断smpl参数是否是一个正常的人体，那么自然就不需要与输入2D image对应的3D ground truth数据来训练这个判别器判断参数的正确性。由于事先知道我们预测的latent space的意义，所以作者将整个discriminator分解为pose discriminator和shape discriminate。作者又进一步将pose discriminator分解为对每个关节点的判别器（23个，这些判别器的作用在于限制每个关节点的旋转角度）和一个对所有关节点整体做判断的判别器（这个判别器作用在于判断所有关节点的分布关系)。 作者认为因为这个网络结构不存在刻意去欺骗Discriminator的行为，所以不会产生一般GANs网络会产生的mode collapse现象。 loss函数 $L = \lambda(L_{reproj} + \mathbb{1}L_{3D}) + L_{adv} $，$\lambda$是用来控制两个目标函数的相对权重的。若输入的2D图像有对应的ground truth 3D数据时，$\mathbb{1}$的值就为1,否则就是0。 $L_{reproj} = \sum \limits_{i}||v_{i}(x_{i}-\hat{x_{i}})||_{1}$,其中$\hat{x_{i}} = s\Pi(RX(\theta,\beta)) + t$。 $R$表示旋转操作，$\Pi$表示正交投影，$s，t$分别表示Scale和translation。 $L_{3D} = L_{3D joints} + L_{3D smpl}$ $L_{joints} = ||(X_i-\hat{X_i})||_2^2$ $L_{smpl} = ||[\beta_i,\theta_i]-[\hat{\beta_i},\hat{\theta_i}]||_2^2$ $minL_{abv}(E) = \sum_i\mathbb{E}_{\Theta ~ pE}[(D_i(E(I)-1)^2]$ $minL(D_i) = \mathbb{E}_{\Theta~pdata}[(D_i(\Theta)-1)^2]+\mathbb{E}_{\Theta ~ pE}[D_i(E(I)^2]$ 很典型的GANs网络结构的Loss函数，其中$\Theta$表示真实的smpl参数。（这个loss函数真的能够限制每个关节的旋转角度和整体关节的分布情况么？？表示很难理解,欢迎提出新的见解~) Experiments对于2D image dataset,作者使用了LSP,LSP-extended,MPII和MS COCO，训练数据量分别是1k,10k,20k和80k。2D image和3D dataset作者使用了Human 3.6M和MPI-INF-3DHP。而用作训练discriminator的SMPL真实数据是: CMU, Human3.6M training set 和the PosePrior dataset,训练数据量分别是390k,150k,180k。 作者在Human 3.6M上采用了两种评价标准： mean per joint position error (MPJPE) Reconstruction error, which is MPJPE after rigid alignment of the prediction with ground truth via Procrustes Analysis. 相较于第一种，Reconstruction error能排除全局误差的影响，更好的比较3D skeleton. 作者发现即使拥有较高的MPJPE，文中的方法生成的3D模型从视觉效果上看，也不错。 作者在MPI-INF-3DHP上采用了两种评价标准： Percentage of Correct Keypoints(PCK) thresholded at 150mm Area Under the Curve(AUC) over a range of PCK thresholds]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维人体重建</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>深度学习</tag>
        <tag>SMPL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(一)Learning Category-Specific Mesh Reconstruction From Image Collections]]></title>
    <url>%2F2019%2F03%2F08%2Fed6ad9d5%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Learning Category-Specific Mesh Reconstruction from Image Collections论文链接 Github链接 Contribution 单图重建 纹理渲染 训练过程无需3d ground truth数据 Method训练数据预处理本次实验用到的数据集是CUB-200-2011,提供了200种鸟类，共计11788张图片。每张图片附含了15个关键点的标记（头部，背部，脚等）、一张Mask和Bounding box。作者首先将这些数据导入Matlab当中，首先对每组关键点进行零均值处理，接着计算平均模型$\hat{S}$。 接着利用所有关键点进行structure from motion变换，得到每张图片相对于平均模型$\hat{S}$的rot,Translation和Scale三个参数。 网络训练作者用到了三个网络来达到重建效果，分别是文中所用主体网络,Neural Mesh Render和Perceptual loss。 论文中所用网络简介 首先网络接受一张图片，经过encoder之后变为了一个200d的shape feature f（注：encoder是一个经过imagenet预训练的resnet-18接上两个全连接层）。之后将得到的f送入三个网络,分别是ShapePredictor,CameraPredictor和UVMapPredictor. 其中ShapePredictor是预测形状变化$\Delta{V}$,最后的形状预测结果就是$V = \hat{V} + \Delta{V}$。CameraPredictor是预测rot,Translation和Scale三个参数的。值得一提的是rot参数,它与以前见到的旋转矩阵不同，rot$\in \mathbb{R}^4$,是一个四元数,可以用来表示三维空间中点的旋转。 TexturePredictor 作者提出的纹理预测思路比较巧妙。首先在数据预处理过程中的$\hat{S}$与网络训练过程的$\hat{V}$并不一致，实际上$\hat{V} =\mathcal{P}(\hat{S})$，$\mathcal{P}$表示将一个点数为642，面片数为1280的二十面体投影至$\hat{S}$。在将二十面体投影至$\hat{S}$前，作者对二十面体的点，面做了一次重新排序，使他们的顺序按照独立点，左侧点，右侧点排列。而投影操作$\mathcal{P}$作者在代码注释中写了这么一句话： 1234def triangle_direction_intersection(tri, trg): Finds where an origin-centered ray going in direction trg intersects a triangle. Args: tri: 3 X 3 vertex locations. tri[0, :] is 0th vertex. 多边形相当于二十面体，而中间的三角形就相当于$\hat{S}$,其边就可以看作三角面片。投影操作就是求解沿着二十面体点的方法，求与三角片面的交点。 有了上面的基础，作者就认为由于所有的预测3D模型都是从同一个平均模型在保持拓扑变化的基础上变化得到的原因，所以每个所预测3D模型的点语义都是一致，即编号为1的点若代表嘴巴，则所有预测模型中编号为1的点就是所预测的嘴巴那个点。那么对于所预测的纹理图片来说，只要知道了最原始的那个二十面体的UV图，便可以对预测3D模型进行渲染(rendering)。 Loss函数介绍 注1：$\mathcal{R}$（）和$G$（）分别表示渲染(rendering)和双线性取样(bilinear sampling),其中$\mathcal{R}$（*）引用自Hiroharu Kato, etc. Neural 3D Mesh Renderer 注2：$L_{texture}$函数表示的percetual loss,较传统的pixel loss相比更能从人的感知角度来评价两幅图像的相似度 引用自Zhang,R etc. The unreasonable effectiveness of deep networks as a perceptual metric. In CVPR 2018 $L_1 = L_{reproj} + L_{mask} + L_{cam} + L_{smooth} + L_{def} + L_{vert2kp}$ $L_{reproj} = \sum_{i}^{}||x_{i}-\tilde{\pi}_{i}(AV_{i})||_{2}$，其中 $\tilde{\pi}（*）$表示投影操作，其值是从structure-from-motion中获得参数 $L_{mask} = \sum_i||S_{i}-\mathcal{R}(V_{i},F,\tilde{\pi}_{i})||_{2}$，$S_{i}$表示ground-truth的Mask。 $L_{cam} = \sum_{i}||\tilde{\pi}_{i}-{\pi}_{i}||_{2}$，$\pi_{i}$表示估计的相机旋转参数，由于旋转参数是由四元数表示，那么就是利用hamilton_product来求解估计与实际的误差 $L_{smooth} = ||LV||_{2}$，$L$表示Laplacian光滑，其目的是为了最小化平均曲率 $L_{def}=||\Delta{V}||_{2}$，这个是一个正则项 $L_{vert2kp} = \frac{1}{|K|}\sum_{k}\sum_{v}-A_{k,v}logA_{k,v}$，这个是一个k*V的矩阵，每一行表示一个特征点，而每一列表示这个特征点在各个点的概率分布。初始化是，每个特征点在各个点上的概率分布是一致的，进过迭代之后，作者期望形成一个类似与one-hot的矩阵。 $L_2 = L_{texture} + L_{dt}$ $L_{texture} = \sum_{i}dist(\mathcal{S_{i}}\bigodot\mathcal{I_{i}},\mathcal{S_{i}}\bigodot\mathcal{R}(V_{i},F,\tilde{\pi_{i}},I^{uv}))$ $L_{dt} = \sum_{i}\sum_{u,v}G(\mathcal{D_{S_{i}};F_{i}}(u,v)$， $\mathcal{D_{S_{i}}}$表示对于每一个Mask的distance transform。 TexturePredictor会输出一个texture flow，也就是$\mathcal{F} \in \mathbb{R}^{H_{uv} * W_{uv} * 2}$,其中$H_{uv}*W_{uv}$分别表示UVmap的长和宽，而$\mathcal{F}(x,y)$对应的就是input image(x,y)。最后的UVmap就是$I^{uv} = G(I;\mathcal{F})$。而这个texture flow不是神经网络的直接输出产物，而神经网络直接输出的是一个表示体现了UV图和Img的对应位置的关系，而其还是会与另一个UVsample进行取样。而UVsample表示的是UV图与$\hat{V}$上各个点的一个对应关系。这两个东西结合在一起后就可以体现UVmap、$\hat{V}$、img三者之间的关系。 Experiments]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维动物重建</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>纹理渲染</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F07%2F4a17b156%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>

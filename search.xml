<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[(五)Learning View Priors for Single-View 3D Reconstruction]]></title>
    <url>%2F2019%2F08%2F19%2F54bbc8cd%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Learning View Priors for Single-view 3D Reconstruction论文链接 Contribution 提出了新的视角先验方法，该方法弥补了对称先验的不足，虽然重建物体大多是符合对称先验的，但是在拍摄照片的那一瞬间，由于拍摄角度和物体本身发生的一些形变等原因，使得待重建的物体本身并不是对称，这样便会使得重建效果不佳。 提出了internal pressure,即点可在与面片垂直的外方向尽可能膨胀。使得重建的物体更加饱满，看起来更加真实。 Methond1本文主要介绍其中的单图重建工作，方法可以很好的泛化为多图重建方法。 View prior 其核心Loss函数为： $\mathcal L_{r}(x,v) = \sum_{i=1}^{N_0} \mathcal L_{v}(P(R(x_{i1}),v_{i1}),x_{i1}) \tag{1}$其中$R\left (* \right )$表示为重建函数，$P \left (* \right )$表示将重建好的三维物体按照$v_{i1}$视角投影会二维平面。而$\mathcal L_{v}$为衡量$x_{i1}$和$v_{i1}$视角下的重投影结果之间的误差（a function that measures the difference between two views）。 $\mathcal L_{s_1}(x_s,\hat x_s) = \sum_{i=1}^{N_s}\left( 1- \frac{x^i_s \cdot \hat x^i_s}{|x^i_s||\hat x^i_s|}\right) \tag{2}$ $\mathcal L_{s_2}(x_s,\hat x_s) = 1 - \frac{|x_s \bigodot \hat x_s|_1}{|x_s+\hat x_s + x_s \bigodot \hat x_s|_1} \tag{3}$ $\mathcal L_v = \mathcal L_s + \lambda_c \mathcal L_c \tag{4}$ 其中$\mathcal L_c$为perceptual loss输入为RGB图片。$\mathcal L_s$的输入为剪影。$\mathcal L_{s_1}$是计算两个mask之间的余弦距离（可能是代表相似度？？），作者使用了多尺度mask,$N_s$代表下采样的次数。 $\mathcal L_{s_2}$为IOU（intersection over union)函数，为的是约束其剪影尽可能重叠。正如上面的$\mathcal L_r(x,v)$所介绍的那也，以上的所有函数都是为了评价两张图片($x_s,\hat x_s$)之间的相似度,其中前者为ground-truth图片,后者为预测的图片。在此基础上，作者使用GAN网络来学习训练数据集中待重建物体不同视角的先验知识,而上面这个Loss函数是用来限制生成器的，根据原图和重投影图片之间的误差不同，来更新生成器的参数，以期获得更好的重建效果。 可以看到，对于椅子，使用作者提出视角先验的方法，重建的结果从各个视角看上去都更加符合实际情况。 基于这种情况，作者使用判别器来给生成器提供不同视角的重建信息。 $\mathcal L_d(x_{ij},v_{ij}) = -log(Dis(P(R(x_{ij}),v_{ij}),v_{ij}))-\sum_{v_u \in \mathcal V ,, v_u \neq v_{ij}} \frac{ log(1- Dis(P(R(x_{ij}),v_{u}),v_{u}))}{|\mathcal V -1|} \tag{5}$ 整个网络的输入为单张RGB的图片，生成器首先依据图片重建出一个对应的三维模型，再根据这张图片所对应的相机视角反投影回二维平面，获得的图片可以记为$x_{gt}$。接着，在从$\mathcal V$中选取任一与之前的相机视角不一致的的相机视角，并依据此相机视角，再一次投影会二维平面。这是，第一次投影所得的$x_{gt}$就可以当做真实数据，第二次投影所得就可以当做加数据，一同送入判别器进行判断。作者在文中提到，所有的视角都是存在于训练集中的（$\mathcal V$ be the set of all viewpoints in the training dataset）。这是否意味着对于一些在训练集中不存在的视角，该网络还是难以学习到其形状？ Internal Pressure 这是一个较弱的先验知识，这个是受启发与Visual hull重建技术。通俗的来讲，就是使重建结果，在不改变投影误差的限制下，尽可能的朝着某个方向膨胀。作者采用的方法就是对每个点施加了一个沿着所在面片法向量的梯度（Concretely, we add a gradient along the normal of the face for each vertex of a triangle face. Let p i be one of the vertices of a triangle face, and n be the normal of the face）。为了实现这一效果，需要添加一个函数，该函数需要满足:$\frac{\partial \mathcal L_{p}(p_i)}{\partial p_i} = -n \tag{6}$ 即关于$p_i$点的偏导等于该点所在的面片的法向量$\overrightarrow n$。文中没有提及具体的函数选择。 Experiments 作者在文中提到，对于手机，沙发，飞机等物体。由于其形状单一，获取的多视角先验可以很好的解决在不可见视角下，重建效果差的问题。但是对于灯(lamp）这类物体，由于其形状的多样性，现有的网络并不能很好解决这个问题。]]></content>
      <categories>
        <category>三维重建，刚性物体重建</category>
      </categories>
      <tags>
        <tag>深度学习，视角先验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(四)Canonical Surface Mapping via Geometric Cycle Consistency]]></title>
    <url>%2F2019%2F08%2F09%2F6021dd11%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Canonical Surface Mapping via Geometric Cycle Consistency 论文链接 Github链接 Contribution 利用三维Template做为中介，弱化了实现稠密的，准确的多张图片对其所需的监督条件 Method Preliminaries 作者利用两个参数将三维template平面化，$\overrightarrow u(u,v)$ 其中$u \in (0,1) v \in (0,1)$ 并且定义$ \phi(\overrightarrow u) $为恢复三维$(x,y,z)$的操作。作者定义$\mathcal C \equiv f_\theta(I)$ 其中$f_\theta(I)$表示网络参数，作者将网络的预测行为抽象定义为$\mathcal C$ Geometric Cycle Consistency Loss $ \mathcal L_{cyc} = \sum_{p\in I_f} ||\overline p - p ||_{2}^{2} \quad;\quad \overline p = \pi(\phi(\mathcal {C}[p])) \tag{1}$ 上面这个就是作者提出的一致性Loss,很好理解，就是像素$p$先经过网络映射在三维template上，接着投影回原图，两者之间的距离要尽可能小 Incorporating Visibility Constraints 由于相机视角的原因，任何物体都会存在一个自遮挡的问题，而这个问题在预测图片稠密映射时就会产生一定的干扰。比方说，从正面看过去，一只鸟类的喙很有可能与其尾巴处于一条线上，所以当投影至二维像平面式，尾巴上某点就会被喙给遮挡。在这种情况下，如果映射关系预测网络将图片上的喙映射在三维template的尾巴上，最后投影回像平面计算$\mathcal L_{cyc} $的时候，依旧Loss会很小。所以作者就提出了： $\mathcal L_{vis} = \sum_{p \in I_f}max(0,z_p - D_{\pi}[\overline p]) \tag{2}$ 其中$D_{\pi}[\overline p]$是三维template在对应相机参数下的深度图。 Mask Re-projection Loss &amp; Multi-Hypothesis Pose Prediction 作者为了彻底摆脱对基准相机参数的需求，又新增了一个网络$g_{\theta^{‘}}$ 这个网络是用来预测相机，并且为了避免局部最小解(local minima)，作者利用预测多个相机参数来达到这个目的。最后，作者使用一个约束来指导相机参数$\pi$的预测 $ \mathcal L_{mask} = ||f_{render}(S,\pi) - I_f||^2 \tag{3}$ 其中$f_{render}$是在预测相机参数下，三维template投影至二维的Mask。 正如上面提到，作者会一次性预测多个相机参数，所以有${ (\pi_i,c_i)} \equiv g_{\theta’}(I) \quad i=1..8 $,其中$c_{i}$表示每个预测结果的概率是多少 。 最后，总的约束函数为：$\mathcal L_{tot} = \mathcal L_{div}(g_{\theta’}(I)) + \sum_{i=1}^{N_c}c_i(\mathcal L_{cyc}^i + \mathcal L_{vis}^i + \mathcal L_{mask}^i) \tag{4}$ 网络结构如图所示，对于2D-3D的映射关系预测，作者是利用一个U-Net结构（红色标注），输出为$B4H*W$的特征图，其中[:,:3,:,:]表示的是预测的三维坐标。[:,3,:,:]表示的是预测的mask。$H,W$分布代表输入图像的长和宽。$g_{\theta’}$表示相机参数预测网络，具体上，作者首先用ResNet18提取图片特征，接着送入FC层预测相关相机参数。作者在相机参数的预测网络上，有很多设定还是十分有趣的，有兴趣可以找来看下。 大致映射 之所以论文里提到approximate这个词呢，是因为，从代码上来看，$f_{\theta’}$的输出$BHW*4$中关于2D pixel to 3D vertex的映射并不一定是在template mesh上，对于网络输出的一对UV坐标(u,v)，作者首先是找到离这对坐标最近的面片是哪一个，接着计算该点关于这个面片的重心坐标(barycentric coordinate),最后根据重心坐标和面片三个顶点的位置坐标，得到最终的3D坐标，同时也确保了这个坐标在template mesh上。可以想象一个四面体（A-BCD），网络预测的3D坐标可能是A点，那么作者就是找到A点在BCD上的对应点。 Experiments 咋一看结果挺好，但是不知道作者为什么没有把CMR带基准相机参数的评测结果放出来，从消融实验的结果来看，在不利用预测相机参数的情况下，CSM结果并没有比不使用预测相机参数的CMR好太多，甚至在cars类别上的评分还低些]]></content>
      <categories>
        <category>论文阅读</category>
        <category>Surface Mapping</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(三)Paperset:how to Extract Geometric Features From Spatial Layout of Points]]></title>
    <url>%2F2019%2F06%2F06%2Fa24b1b46%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Paper set: how to extract geometric features from spatial layout of points [1]TOG2018 Dynamic Graph CNN for Learning on Point Clouds [2]CVPR2018Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling [3]AAAI2019MeshNet: Mesh Neural Network for 3D Shape Representation [4]CVPR2019Structural Relational Reasoning of Point Clouds [5]CVPR2019Relation-Shape Convolutional Neural Network for Point Cloud Analysis Dynamic Graph CNN for Learning on Point CloudsDescription:作者认为Pointnet和Pointnet++均只是探究了点云局部的独立的，本身的几何结构（Pointnet尽管是将一个完整点云分割为若干子点云来分别计算几何结构，但其还是处于local sacle的程度），并没有得到自己与其他点，其他子点云之间的关系。作者利用图神经网络在处理不规则数据上的优势，在抽取点云中各个点之间的几何关系，从而弥补了之前方法的不足 Contribution:作者提出了一种可以在点云处理中使用的操作：EdgeConv。并且通过大量的实验证明了改操作可以更好的在提取点云局部几何特征的同时保持排列不变性。 Method:作者在relate work-&gt;Geometric Deep learning中提到A key difficulty,however,is that the Laplacian eigenbasis is domain-dependent;thus, a filter learned on one shape may not generalize to others. 就是对于图卷积网络来说，基于频域的卷积其泛化性不如在空域中的卷积。 EdgeConv 假设点云点数为N,同时每个点上的特征维度为F,那么就有$X = {x_{1},…,x_{n}} \subseteq R^{F}$ 。对于每个点上的特征向量的计算，选取在特征空间上与中心点最近的K近邻点(k-nearest neighbor) 表示为$\mathcal G = (\mathcal V, \varepsilon)$ 其中$\mathcal V={1,…,n }$代表点，$\varepsilon$表示中心点与其K近邻的边。那么EdgeConv的公式可以表示为： $ x_{i}^{‘} = \square h_{\theta} (x_{i},x_{j})$ &emsp;&emsp;$s.t. j:(i,j) \in \varepsilon$ $x_{i}$表示在i th点上的特征向量 其中$\square$ 代表channel-wise的对称aggregation操作，例如叠加或求最大值（to keep permutaion invariance)，每层EdgeConv对每个点$x_{i}$都会更新其特征向量本文采用的Max Aggregation方法 Choice of $h_{\theta}$ $h_{\theta}(x_{i},x_{j}) = \theta_{j}x_{j}$ 这就相当于普通的CNN卷积了，$x_{i}$就相当于卷积核中间的那个点，$x_{j}$是其周围的点，$\theta$是可学习参数 $h_{\theta}(x_{i},x_{j}) =h_{\theta}(x_{i})$ 这就是Pointnet的处理方式，只独立的考虑一个点 $h_{\theta}(x_{i},x_{j}) =h_{\theta}(x_{i}-x_{j})$ 该方法作者认为只考虑了编码局部信息，失去了全局形状结构 $h_{\theta}(x_{i},x_{j}) =h_{\theta}(x_{i},x_{i}-x_{j})$ 弥补了上一个方法的不足，也是本文所采取的方法 Dynamic graph update 在每一层EdgeConv之后，作者都会计算一个根据新得到的每个点的特征向量，计算一次KNN,并用这个新的KNN Graph来计算下一层的特征向量。 Implementation 注意EdgeConv层的维度 Experiment 首先当然是取得了state-of-art的成绩。有趣的，作者发现对于近邻K的选取，不是越大越好，对于不同的任务，都有一个峰值K使得效果最好，作者认为过大的K会使得欧式距离不能很好的估计测地距离（geodesic distace)会摧毁几何信息。同时作者发现K的选取跟点的密度有关 Mining Point Cloud Local Structures by Kernel Correlation and Graph PoolingDescription:这篇文章与[1]是同一时期的论文，但是在ModelNet40上的分类和ShapeNet上的分割都略微不如[1],但其思想是值得借鉴的。 作者利用在04年ECCV上的一篇关于point cloud registration工作上的技术kernel Correlation来对局部几何信息进行编码，可以理解为2D CNN中卷积核(kernel)的一种特殊的3D变种，其会对具有不同结构信息的点产生不同程度的响应(response)。同时作者也提出了一种利用K nearest neighbor graphs(KNNG)的新的特征聚合方法（feature aggregation)来利用高维结构特征推断集合信息。 Contribution:作者提出了借用kernel correlation的思想来简化求解local structural feature Method: Local Geometric Properties Surface Normal: 作者在论文中提到可以用PCA求解的minimum variance direction作为局部的法向量，原话是:surface normal can be estimated by principal component analysis on data covariance matrix of neighboring points as the minimum variance direction Covariance Matrix(协方差矩阵): 作为二阶局部描述子，它能提供比normal更加多的信息量。但是作者认为normal与Covariance Matrix作为手工设定的特征，其描述局部结构特征的能力有限，因为不同形状的几何结构可能产生相似的normal或者covariance matrix，这就给后续工作带来巨大的噪音。 Kernel Correlation: Kernel Correlation可以帮助用来判断两个Point cloud之间的相似度（similarity)。在point cloud registraion当中，一个point cloud作为reference,另一个作为source。source要不断的进行刚性/非刚性变换以使得kernel correlation的响应达到最大。也就是说，响应大的时候，两者的几何结构是非常相似的，那么这种性质就可以用来构造卷积核。 Learning on Local Geometric Structure point cloud registration 中的kernel corelation是固定一个template(reference),试图找到一个变换（transformation)使得source+transformation的结果与refrence的响应最大。而本文作者对此进行了改进，在整个训练过程中，template(reference）不再被固定，而是通过反传误差，不断学习到一个合适的reference(这里可以看成不断调整初始reference中点的位置），从而使reference与source的响应达到一个水准，这个水准能与网络其余结构一起产生预期的预测效果。论文原话是： Under this setting, the learning process can be viewed as finding a set of reference/template points encoding the most effective and useful local geometric structures that lead to the best learning performance jointly with other parameters in the network. kernel corelation的计算公式： $KC(\mathcal k,x_{i} ) = \cfrac{1}{\mathcal N(i)}\sum\limits_{m=1}^{M}\sum\limits_{n \in \mathcal N(i)}K_{\sigma}(\mathcal k_{m},x_{n}-x_{i}) \tag {1} $ $\mathcal k_{m}$表示kernel中的m个可学习的点，$\mathcal N(i)$ 表示anchor point $x_{i}$的周围K近邻，而$K_{\sigma}(*,*)$表示核函数，也就是公式$(2)$所示。公式$(1)$大体就是对于某个corelation kernel来说，其中每个点$\mathcal k_{m}$都会与$\mathcal N(i)$中包括的点一起作为公式$(1)$的输入，也就是会计算m*k次。$K_{\sigma}(k,\delta) = exp(-\cfrac{||k-\delta||^2}{2\delta^2}) \tag {2}$ 分子表示欧式距离，分母$\delta$表示核的宽度，它控制点之间距离的影响（这里我也不是太懂）。文中的$\delta$被设置为the average neighbor distance in the neighborhood graphs over all training point clouds。 可以设置$L$个kernel corelation,这个$L$与卷积网络中的output channel类似。 Learning on Local Feature Structure 核心思想就是每个点预期近邻点的几何结构是一致，将各个点与其自身的近邻点做特征向量的aggregation有利于使得网络具有更好的效果和鲁棒性。作者采用了average pooling和max pooling， 其中max pooling为 $\mathcal Y(i,k) = \max\limits_{n \in \mathcal N(i)} X(n,k) \tag{3}$ k表示第i个点的特征向量的第k个维度，特征向量维度与kernel corelation的核数量一致 average pooling为：$Y=D^{-1}WX \tag{4}$$W$为邻接矩阵，$D$为度矩阵(degree matrix) 本文采用的是Max pooling作为Aggregation方法 Experiment: 作者通过Ablation study从Effectiveness of Kernel Correlation,Symmetric Functions,Effectiveness of Local Structures三个方面证明了本文所提方法的有效性： 但我觉得存在一个问题:在Effectiveness of Kernel Correlation试验中，每个点的Normal是使用PCA方法求出的，然后再与坐标Catenate.那么所求的Normal与真实的值相差多少呢?这个误差是否会影响实验结果呢？如果改用更加准确的Normal求解方法，是否能提高，甚至超过Kernel Correlation方法呢？ Conclusion:这篇工作的根本目的在于探究一种较PointNet++更为简单的局部几何计算子，从而在节省算力的同时，还能提高准确度。但是其本质上是以一种更加简单的方式回答what type of the point，其没有解决点与点之间是什么关系，换句话说，PointNet++与KC-Net的本质与[1]中的DG-CNN是不同的，这可能也是造成他们效果差异的原因。 MeshNet: Mesh Neural Network for 3D Shape RepresentationDescription: 本文提出一种基于Mesh的3D shape特征抽取方法,直接将对于Mesh-based 3D shape的分类准确率从68.3%提升到91.9%。作者将单独的一个Face视为一个Unit,这样做可以消除Mesh固有的irregular，因为每一个face有且只有3个面与其相邻（作者这里用的是no more than，我认为不对)。其次，作者将一个Face的Feature分为spatial feature和structural feature,这样做有利于是Face Feature具有更加明确的指向性，使网络更容易学习到有用的Feature Method: Input: 作者将Face-unit进行了如下分解： Face Information: Center:coordinate of center point Corner:vectors from the center point to three vertices Normal: the unit normal vector Neighbor Information: Neighbor Index: indexes of the connected faces(filled with the index of itself if the face connects with less than three faces) 如果面片形成的是一个闭集，那么必然是一个面片有三个相邻面的 Spatial and Structural Descriptor spatial descriptor: 输入只是Center坐标，简单的经过MLP得到结果 Structural descriptor:face rotate convolution $g(\cfrac{1}{3}(f(v_{1},v_{2})+f(v_{2},v_{3})+f(v_{3},v_{1})) \tag{1}$ $f(*,*) : \mathbb R^3 * \mathbb R^3 \rightarrow \mathbb R^{K_1}$ $g(*,*) : \mathbb K^1 \rightarrow \mathbb K^2$可以是任何合法函数（大多都是MLP) 作者将这个操作类比到卷积操作，$f(*,*)$看作卷积核，每次对两个向量做卷积，然后用旋转代替平移，步长为1。$K_1$表示output channel。这个$\cfrac{1}{3}$可以当做对称函数(average pooling)用来消除disorder，保证了permutation invariance。 Structural descriptor: face kernel correlation: 作者利用[2]的kernel corelation将其改为针对face-unit的形式，利用面片的normal代替[2]中点的坐标。由于所有的normal都是单位向量所以作者用$(\theta,\phi)$ 来代替单位向量$(x,y,z)$ 所以有 $\begin{cases} &amp; x=sin \theta cos \phi \\ &amp; y=sin \theta sin \phi \\ &amp; z=cos \phi \tag{2} \\\end{cases}$ 其中：$\theta \in [0,\pi], \phi \in [0,2\pi)$(其实就是仰角和旋转角) $KC(\mathcal k,x_{i} ) = \cfrac{1}{\mathcal |N(i)| \mathcal |M_k|)}\sum\limits_{n \in \mathcal N_i}\sum\limits_{m \in \mathcal M_k}K_{\sigma}(n,m) \tag {3} $ $K_{\sigma}(n,m) = exp(-\cfrac{||n-m||^2}{2\delta^2}) \tag {4}$ 这些都与[2]中的含义一致 Mesh Convolution 如图所示，作者将spatial Feature，structral Feature和Neighbor Index，作为输入送进Combination和Aggregation中，得到新的spatial Feature和structral Feature。 作者在文中提到，之所以不将spatial Feature一起送进Aggregation是因为不想让structral Feature受到点所在空间信息的影响，就如2维卷积的过程中，不会显示的告知卷积核该像素的绝对坐标一样。 图中标识了三种Aggregation方法：Concatenation,Max pooling和Average pooling 作者认为Average pooling是很多强响应被忽略了，从而损失了很多的有用信息。实验表明Concatenation效果最好。 Conclusion: 将Feature拆分为多个更加明确的子特征，可能有助于网络学习 本文有个缺点就是没有考虑显示的考虑面片之间的结构关系，每个面片的特征都只与其本身有关，但不排除在之后了MLP层之后学习到了。 结果显示在移除了MeshConv之后，准确度下降的比重不是最大的 Structural Relational Reasoning of Point CloudsDescription作者首先阐述了Pointnet++的缺点，与[1]中提到的一致–只是探究了局部的结构特征，没有探究这些局部之间的结构也就是依赖关系。但是与[1]不同的是，本文所提出的structural relation network(SRN)探究的是局部与局部的依赖关系(structuraldependencies of local regions)，而[1]是以点为单位的。作者以人类识别物体为例，认为对物体局部之间关系的认知对于理解物体本身具有很大作用 Method $P_i$代表子点云,$u_i \in \mathbb R^d$代表该子点云的局部几何特征，$v_i \in \mathbb R^3$表示该子点云的平均位置用来表示位置信息。作者认为$u_i$对发现重复的局部模式(repetitive local patterns)有帮助，而$u_i$对发现各个局部之间的连接关系(linkage relations)起到关键作用。所有$P_i$的结构关系特征表示为： $y_i = f(\sum \limits_{\forall j}h(g_u(u_i,u_j),g_v(v_i,v_j)) \tag{1}$ $j$表示除了$i$以外的所有子点云。在实现方面，$g_u(·，·),g_v(·，·)$都是由多层感知机(MLP)实现,$f(·，·),h(·，·)$都是由1 $\times$ 1卷积实现。由上面那张图可以看出每个$u_i$都会与其他的所有$u_j$做一次concatenating操作（应该是在channel维度进行拼接） 注意到标黄部分，采用了不同比例的局部特征,作者认为SRN对于不同的子点云获取方法都是鲁棒的。同时红框表示文中提到的residual connection$y$是作为对$u$的补充(complement) Experiment为了证明SRN对3D Shape的局部关系学习具有很好的泛化性，作者在ModelNet40（3D objects)和Scannet(indoor scenes)上做了跨库实验 作者提取分布在两个库上训练的网络所产生的特征向量(feature)之后用线性SVM做分类 作者为了进一步阐述SRN对特征提取的帮助，利用t-SNE(一种对高维数据降维可视化的方法）将所提取的特征进行可视化。发现大多数类别的类间差异都很小。 Relation-Shape Convolutional Neural Network for Point Cloud AnalysisDescription:作者依旧是从探究点与点之间的关系来入手，作者认为从点学习特征具有三点挑战： 需要具备排列不变性(permutation invariant) 对于刚性形变鲁棒，也就是即使点的绝对位置发生改变，只要其结构未变，其特征应该是一致的 效果好 Method $x_i$表示采样点,$x_j \in \mathcal N(x_i)$表示周围邻域点，则该$x_i$上的特征向量$f_{P_{sub}}$可以用来表示这个邻域的结构 $f_{P_{sub}} = \sigma(\mathcal A(\lbrace \tau(f_{x_j}), \forall x_j \rbrace)), d_{ij} &lt;r \forall x_j \in \mathcal N(x_i) \tag{1}$ $\tau(f_{x_j}) = w_{ij} · f_{x_j} = \mathcal M(h_{ij} · f_{x_j}) \tag{2}$ $d_{ij}$是$x_i,x_j$之间的欧式距离。对于上面这个公式，论文的解释是:Here $f_{P_{sub}}$ is obtained by first transforming the features of all the points in $\mathcal N(i)$ with function $\tau$ , and then aggregating them with function $\mathcal A$ followed by a nonlinear activator $\sigma$. 公式(1)相对于论文[1]多了一个$\tau$函数，其余操作一致，$\mathcal A$也是max pooling $\mathcal M$为三层MLP作者之后在试验中证明了3层是较好的选择 Conclusion这篇文章是被录用为CVPR2019的oral.但是其最核心的思想我认为跟[1]是差不多的，作者在文中也用一句话评价了两者的区别DGCNN captures similar local shapes by learning point relation in a high-dimensional feature space, yet this relation could be unreliable in some cases. 我刚看到简直吐血，真·不要碧莲。我认为之所以能被接受为oral原因如下： 优秀的写作，论文从问题到解决办法的提出一气呵成，非常漂亮，很多地方的写作技巧都很微妙，比如上面这句。 实验做得很充分，几乎论证了所有文中提出的tricks。同时效果确实有不小的提升。 具体用到的tricks如下： 利用了hierarchical architecture,在相同的权重下，对不同数量的neighborhoods进行学习，这与[4]是一致的，但文中没有提出这点 $\tau$函数的提出，新学习到的特征向量与老的特征向量做一次点乘,这与ICML2019GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects类似。 相对[1]对每个点求其邻域的特征向量，本文对每个选取的领域求特征 论文采取的对邻域的选取方法也与其他不相同可以看到，在scale=1的情况下，用K-NN选取邻域只有90.5%的准确率，而转为random-PIB之后就提升到了92.2%，这与DGCNN对每个点都求一个结构特征所得到的准确率是一致，但计算量却大大减少了 很可惜，对于核心方法的修改，作者没有给出消融实验的结果，不能看出新增的$\tau$函数对效果的提升有什么帮助。同时对于输入h是否会变也没有给出明确的说明，就符号一致性来说，是不会改变的,[1]中是会改变的]]></content>
      <categories>
        <category>论文阅读</category>
        <category>特征抽取</category>
      </categories>
      <tags>
        <tag>点云</tag>
        <tag>网格</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu18.04装机]]></title>
    <url>%2F2019%2F03%2F24%2F30a4c30f%2F</url>
    <content type="text"><![CDATA[MatlabR2016b 前期准备： R2016b_glnxa64_dvd1.iso，R2016b_glnxa64_dvd2.iso，Matlab 2016b Linux64 Crack.rar 具体流程： 1.挂在ISO文件，尽量挂在空间大点的盘上,并开始安装 12345mkdir matlabISOsudo mount -t auto -o loop R2016b_glnxa64_dvd1.iso matlabISO/这里不用加sudo, 如果加了，会使安装的matlab运行时只能让root用户使用，并且新建的文件普通用户也没有访问权限同时网上有说不能进入matlabISO里去执行installmatlabISO/install 2.选择使用文件安装密钥，同时可以Matlab 2016b Linux64 Crack.rar中获得安装密钥，接着就是默认安装 3.安装到80%左右时，会提示挂载dvd2镜像 12注意这里路径别写错，同时要挂载到同一个文件目录下sudo mount -t auto -o loop R2016b_glnxa64_dvd2.iso matlabISO/ 4.激活Matlab 1234567对文件赋予权限sudo chmod -R 777 llicense_standalone.liccd your/matlab/path/binsh matlab这时会弹出对话框，选择不联网方式激活，并且加载Matlab 2016b Linux64 Crack.rar中的license_standalone.lic将Matlab 2016b Linux64 Crack.rar/glnxa64/bin/glnxa64中的四个文件复制到Matlab中对应位置sudo cp lib* your/matlab/path/bin/glnxa64/ 5.设置matlab快捷方式 sudo gedit /usr/share/application Matlab.desktop 在打开的Matlab.desktop中输入一下内容 [Desktop Entry] Encoding=UTF-8 Name=Matlab 2016b Comment=MATLAB Exec=/your/matlab/path/bin/matlab Icon=/your/matlab/path/toolbox/shared/dastudio/resources/MatlabIcon.png Terminal=true StartupNotify=true Type=Application Categories=Application; Pycharm 前期准备 pycharm-professional-2018.3.5.tar.gz 安装流程 1.放到合适的目录，提取压缩包 2.进入你的压缩路径，运行安装脚本 123cd your/unpack/path/pycharm/bin注意不要加sudosh ./pycharm.sh 3.激活码激活 4.快捷图片 与Matlab操作一致 sougou拼音 前期准备 sogoupinyin_2.2.0.0108_amd64.deb 安装流程 1.安装fcitx 12sudo apt-get install fcitx-binsudo apt-get install fcitx-table 2.配置fcitx 搜索语言支持，将键盘输入法系统设置为fcitx 3.安装sogoupinyin_2.2.0.0108_amd64.deb,并重启电脑 4.点击Ubuntu右上角的键盘，点击设置，将出入搜狗拼音其余输入法删除 cuda 安装流程 1.安装所需依赖项 123sudo apt-get install freeglut3-dev build-essential sudo apt-get install libx11-dev libxmu-dev libxi-dev sudo apt-get install libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev 2.安装CUDA 12345678910111213141516#进入cuda目录cd brl/cuda#安装cudasudo sh cuda_9.0.176_384.81_linux.run --override#当你输入上面这条命令时，终端界面会让你输入一些选择，需要注意的有下面两点：# 1、是否要安装显卡驱动--选择否# 2、决定安装位置--根据自身情况选择安装位置（下面四个补丁也要安装在这个位置上）# 3、除了1以外所有的选择，全部选是（Y）#安装cuda的补丁1sudo sh cuda_9.0.176.1_linux.run#安装cuda的补丁2sudo sh cuda_9.0.176.2_linux.run#安装cuda的补丁3sudo sh cuda_9.0.176.3_linux.run#安装cuda的补丁4sudo sh cuda_9.0.176.4_linux.run 安装完整之后因为没有安装它提示的驱动，所以会提示安装不完整 3.添加环境变量 这里需要注意，当你安装CUDA时，会问你是否更新（创建）软连接，若你是第一次安装CUDA或者你想代替你当前使用的CUDA版本，就要选Y。安装程序就会帮你创建/usr/local/cuda/bin链接到你的CUDA安装位置，若电脑中存在多个CUDA版本时，理论上只要删除这个软连接，再新建一个指向你想使用的CUDA上即可 123456sudo gedit ~/.bashrc在打开的文件中输入export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;另开终端输入source ~/.bashrc cudnncudnn就相当于一个插件，可以随时更换，只要删除老的Cudnn文件即可 安装流程 1.解压cuDnn 2.复制其中文件到你想要使用CUDA中 12345#这里就是cuda,因为你解压缩以后的文件名就是cudasudo cp cuda/include/cudnn.h /usr/local/cuda/include/ #*****sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -dsudo chmod a+r /usr/local/cuda/include/cudnn.hsudo chmod a+r /usr/local/cuda/lib64/libcudnn* 3.进入CUDA更新软连接 123456789由于/usr/local/cuda与你使用的CUDA建立了软连接，所以当你更新这里面的文件时，其他位置的CUDA也会进行相应更新cd /usr/local/cuda/lib64/sudo chmod +r libcudnn.so.7.3.0sudo ln -sf libcudnn.so.7.3.0 libcudnn.so.7sudo ln -sf libcudnn.so.7 libcudnn.sosudo ldconfigsudo ldconfig /usr/local/cuda/lib64/根据cudnn的版本不同libcudnn.so.7.3.0中的数字部分需要修改]]></content>
      <categories>
        <category>常用</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(二)End-to-End Recovery of Human Shape and Pose]]></title>
    <url>%2F2019%2F03%2F09%2Fe6413cd%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;End-to-end Recovery of Human Shape and Pose论文链接 Github链接 Contribution 单图重建 利用SMPL进行重建，并且达到real-time效果 不要求每一个训练的image都要有其对应的3D ground truth Method 作者提出了一个end-to-end的网络来从单张RGB人像恢复其3D形状。作者利用已有的SMPL模型，SMPL模型可由3D relative joint rotation和Shape 来刻画一个人的3D shape。作者认为在以往的人体建模工作当中，有3D joint location来估计一个完整的3D shape 是不鲁棒的。原因有二：1、3D joint location alone do not constrain the full DoF at each joint. DoF意思就是景深，这个跟相机的参数有关。对这个参数的估计错误，可能会导致所估计的3D shape在图像中的显示位置有区别（我自己想的，不一定正确）2、Joints are sparse, whereas the human body is defined by a surface in 3D space. 这个就是Joints的点数过少，不足以约束一个完整的3D human shape。 作者还解决了两个在重建工作的问题。其一就是缺少3D ground-truth数据，同时，已有的3D ground-truth绝大部分是在实验室环境下采集的，其对in the wild的2D图像泛化性很差。其二就是单视角情况下2D-3D mapping的问题，不同的3D shape存在对应一张相同的2D图片，在这些3D shape中，存在一些不正常的shape(may not be anthropometrically reasonable) 网络训练 网络结构 Encoder和Regression 网络首先输入一张图片经过Encoder(Resnet-50)后，输出一个feature$\in \mathbb{R}^{2048}$。之后输入到一个Regressor（3层FC,2048D-&gt;1024D-&gt;1024D-&gt;85D）进行迭代，总共会迭代3次，每次迭代过程中会输入一个cam_para,shape $\beta$,pose $\theta$。将pose和shape输入smpl可以得到本次迭代后预测的模型（包括3D joint location）。由cam_para和3D joint location可以得到2D joint location从而可以计算$L_{reproj}$。由pose(也就是joint rotation)和shape可以计算出$L_{adv}$。若输入的2D image有对应的3D数据，则还可以计算$L_{3D}$。 但是在最后计算loss的时候，只会使用每次迭代时产生的$L_{adv}$，而$L_{3D}$和$L_{reproj}$只会利用最后一次迭代产生的loss。这是由于作者认为若每次迭代产生的loss全部都利用,这容易导致regressor限于局部最优化。 Adversarial Prior Adversarial Prior是用来解决上面提到的生成3D数据不真实情况的一个判别器。由于该判别器的任务是判断smpl参数是否是一个正常的人体，那么自然就不需要与输入2D image对应的3D ground truth数据来训练这个判别器判断参数的正确性。由于事先知道我们预测的latent space的意义，所以作者将整个discriminator分解为pose discriminator和shape discriminate。作者又进一步将pose discriminator分解为对每个关节点的判别器（23个，这些判别器的作用在于限制每个关节点的旋转角度）和一个对所有关节点整体做判断的判别器（这个判别器作用在于判断所有关节点的分布关系)。 作者认为因为这个网络结构不存在刻意去欺骗Discriminator的行为，所以不会产生一般GANs网络会产生的mode collapse现象。 loss函数 $L = \lambda(L_{reproj} + \mathbb{1}L_{3D}) + L_{adv} $，$\lambda$是用来控制两个目标函数的相对权重的。若输入的2D图像有对应的ground truth 3D数据时，$\mathbb{1}$的值就为1,否则就是0。 $L_{reproj} = \sum \limits_{i}||v_{i}(x_{i}-\hat{x_{i}})||_{1}$,其中$\hat{x_{i}} = s\Pi(RX(\theta,\beta)) + t$。 $R$表示旋转操作，$\Pi$表示正交投影，$s，t$分别表示Scale和translation。 $L_{3D} = L_{3D joints} + L_{3D smpl}$ $L_{joints} = ||(X_i-\hat{X_i})||_2^2$ $L_{smpl} = ||[\beta_i,\theta_i]-[\hat{\beta_i},\hat{\theta_i}]||_2^2$ $minL_{abv}(E) = \sum_i\mathbb{E}_{\Theta ~ pE}[(D_i(E(I)-1)^2]$ $minL(D_i) = \mathbb{E}_{\Theta~pdata}[(D_i(\Theta)-1)^2]+\mathbb{E}_{\Theta ~ pE}[D_i(E(I)^2]$ 很典型的GANs网络结构的Loss函数，其中$\Theta$表示真实的smpl参数。（这个loss函数真的能够限制每个关节的旋转角度和整体关节的分布情况么？？表示很难理解,欢迎提出新的见解~) Experiments对于2D image dataset,作者使用了LSP,LSP-extended,MPII和MS COCO，训练数据量分别是1k,10k,20k和80k。2D image和3D dataset作者使用了Human 3.6M和MPI-INF-3DHP。而用作训练discriminator的SMPL真实数据是: CMU, Human3.6M training set 和the PosePrior dataset,训练数据量分别是390k,150k,180k。 作者在Human 3.6M上采用了两种评价标准： mean per joint position error (MPJPE) Reconstruction error, which is MPJPE after rigid alignment of the prediction with ground truth via Procrustes Analysis. 相较于第一种，Reconstruction error能排除全局误差的影响，更好的比较3D skeleton. 作者发现即使拥有较高的MPJPE，文中的方法生成的3D模型从视觉效果上看，也不错。 作者在MPI-INF-3DHP上采用了两种评价标准： Percentage of Correct Keypoints(PCK) thresholded at 150mm Area Under the Curve(AUC) over a range of PCK thresholds]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维人体重建</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>深度学习</tag>
        <tag>SMPL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(一)Learning Category-Specific Mesh Reconstruction From Image Collections]]></title>
    <url>%2F2019%2F03%2F08%2Fed6ad9d5%2F</url>
    <content type="text"><![CDATA[&lt;论文阅读&gt;Learning Category-Specific Mesh Reconstruction from Image Collections论文链接 Github链接 Contribution 单图重建 纹理渲染 训练过程无需3d ground truth数据 Method训练数据预处理本次实验用到的数据集是CUB-200-2011,提供了200种鸟类，共计11788张图片。每张图片附含了15个关键点的标记（头部，背部，脚等）、一张Mask和Bounding box。作者首先将这些数据导入Matlab当中，首先对每组关键点进行零均值处理，接着计算平均模型$\hat{S}$。 接着利用所有关键点进行structure from motion变换，得到每张图片相对于平均模型$\hat{S}$的rot,Translation和Scale三个参数。 网络训练作者用到了三个网络来达到重建效果，分别是文中所用主体网络,Neural Mesh Render和Perceptual loss。 论文中所用网络简介 首先网络接受一张图片，经过encoder之后变为了一个200d的shape feature f（注：encoder是一个经过imagenet预训练的resnet-18接上两个全连接层）。之后将得到的f送入三个网络,分别是ShapePredictor,CameraPredictor和UVMapPredictor. 其中ShapePredictor是预测形状变化$\Delta{V}$,最后的形状预测结果就是$V = \hat{V} + \Delta{V}$。CameraPredictor是预测rot,Translation和Scale三个参数的。值得一提的是rot参数,它与以前见到的旋转矩阵不同，rot$\in \mathbb{R}^4$,是一个四元数,可以用来表示三维空间中点的旋转。 TexturePredictor 作者提出的纹理预测思路比较巧妙。首先在数据预处理过程中的$\hat{S}$与网络训练过程的$\hat{V}$并不一致，实际上$\hat{V} =\mathcal{P}(\hat{S})$，$\mathcal{P}$表示将一个点数为642，面片数为1280的二十面体投影至$\hat{S}$。在将二十面体投影至$\hat{S}$前，作者对二十面体的点，面做了一次重新排序，使他们的顺序按照独立点，左侧点，右侧点排列。而投影操作$\mathcal{P}$作者在代码注释中写了这么一句话： 1234def triangle_direction_intersection(tri, trg): Finds where an origin-centered ray going in direction trg intersects a triangle. Args: tri: 3 X 3 vertex locations. tri[0, :] is 0th vertex. 多边形相当于二十面体，而中间的三角形就相当于$\hat{S}$,其边就可以看作三角面片。投影操作就是求解沿着二十面体点的方法，求与三角片面的交点。 有了上面的基础，作者就认为由于所有的预测3D模型都是从同一个平均模型在保持拓扑变化的基础上变化得到的原因，所以每个所预测3D模型的点语义都是一致，即编号为1的点若代表嘴巴，则所有预测模型中编号为1的点就是所预测的嘴巴那个点。那么对于所预测的纹理图片来说，只要知道了最原始的那个二十面体的UV图，便可以对预测3D模型进行渲染(rendering)。 Loss函数介绍 注1：$\mathcal{R}$（）和$G$（）分别表示渲染(rendering)和双线性取样(bilinear sampling),其中$\mathcal{R}$（*）引用自Hiroharu Kato, etc. Neural 3D Mesh Renderer 注2：$L_{texture}$函数表示的percetual loss,较传统的pixel loss相比更能从人的感知角度来评价两幅图像的相似度 引用自Zhang,R etc. The unreasonable effectiveness of deep networks as a perceptual metric. In CVPR 2018 $L_1 = L_{reproj} + L_{mask} + L_{cam} + L_{smooth} + L_{def} + L_{vert2kp}$ $L_{reproj} = \sum_{i}^{}||x_{i}-\tilde{\pi}_{i}(AV_{i})||_{2}$，其中 $\tilde{\pi}（*）$表示投影操作，其值是从structure-from-motion中获得参数 $L_{mask} = \sum_i||S_{i}-\mathcal{R}(V_{i},F,\tilde{\pi}_{i})||_{2}$，$S_{i}$表示ground-truth的Mask。 $L_{cam} = \sum_{i}||\tilde{\pi}_{i}-{\pi}_{i}||_{2}$，$\pi_{i}$表示估计的相机旋转参数，由于旋转参数是由四元数表示，那么就是利用hamilton_product来求解估计与实际的误差 $L_{smooth} = ||LV||_{2}$，$L$表示Laplacian光滑，其目的是为了最小化平均曲率 $L_{def}=||\Delta{V}||_{2}$，这个是一个正则项 $L_{vert2kp} = \frac{1}{|K|}\sum_{k}\sum_{v}-A_{k,v}logA_{k,v}$，这个是一个k*V的矩阵，每一行表示一个特征点，而每一列表示这个特征点在各个点的概率分布。初始化是，每个特征点在各个点上的概率分布是一致的，进过迭代之后，作者期望形成一个类似与one-hot的矩阵。 $L_2 = L_{texture} + L_{dt}$ $L_{texture} = \sum_{i}dist(\mathcal{S_{i}}\bigodot\mathcal{I_{i}},\mathcal{S_{i}}\bigodot\mathcal{R}(V_{i},F,\tilde{\pi_{i}},I^{uv}))$ $L_{dt} = \sum_{i}\sum_{u,v}G(\mathcal{D_{S_{i}};F_{i}}(u,v)$， $\mathcal{D_{S_{i}}}$表示对于每一个Mask的distance transform。 TexturePredictor会输出一个texture flow，也就是$\mathcal{F} \in \mathbb{R}^{H_{uv} * W_{uv} * 2}$,其中$H_{uv}*W_{uv}$分别表示UVmap的长和宽，而$\mathcal{F}(x,y)$对应的就是input image(x,y)。最后的UVmap就是$I^{uv} = G(I;\mathcal{F})$。而这个texture flow不是神经网络的直接输出产物，而神经网络直接输出的是一个表示体现了UV图和Img的对应位置的关系，而其还是会与另一个UVsample进行取样。而UVsample表示的是UV图与$\hat{V}$上各个点的一个对应关系。这两个东西结合在一起后就可以体现UVmap、$\hat{V}$、img三者之间的关系。 Experiments]]></content>
      <categories>
        <category>论文阅读</category>
        <category>三维动物重建</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>纹理渲染</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F07%2F4a17b156%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
